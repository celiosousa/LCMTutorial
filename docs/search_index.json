[["index.html", "Supervised Land Cover Classification with Google Earth Engine Introduction and overview", " Supervised Land Cover Classification with Google Earth Engine Celio Sousa (celio.h.resendedesousa@nasa.gov) and Miroslav Honzak (mhonzak@conservation.org) 2024-10-01 Introduction and overview Welcome to Supervised Land Cover Classification with Google Earth Engine! This online classification tutorial is a byproduct of the work conducted under the NASA-Conservation Partnership, signed in 2018 with the overarching goal of assisting African nations to account for their ecosystem services and natural capital. More specifically, we aimed to develop repeatable methodologies to map land cover and ecosystem extent, meet international standards for ecosystem accounting, and satisfy the requirements of a broad range of nation-specific decision support needs. This tutorial will include the steps taken through two work-streams to develop ecosystem extent maps: Figure 0.1: NASA- and Conservation International-led work-streams: land cover and plant dissimilarity for ecosystem mapping. This tutorial is broadly organized in four parts: These parts are divided into Map production and Map application tiers and whether or not each section requires basic remote sensing knowledge. An overview of the tutorial structure is shown below: The first half of the TIER 1 (PART 1) is designed to provide you with the basic remote sensing knowledge to understand the concepts that will be discussed in the following sections. Also, it aims to take you from being a complete Earth Engine novice to being fairly familiarized with its basic functionalities. Those who are almost entirely unfamiliar with remote sensing data, or are almost entirely unfamiliar with programming, or both, are encouraged to start at the beginning (Programming and Remote Sensing Basics). This section assumes that the user has no remote sensing background and programming experience, and begins with the most fundamental first steps. The second the half of TIER 1 (PART 2), presents you with a guided tour of the world of satellite imagery classification using Google Earth Engine! In this section we explore the overall approach of image classification using real life and real location examples: from the pre-classification steps of preparing the imagery to post-classification procedures to enhance your classification outputs. The TIER 2 is designed to apply all the concepts and techniques covered in PART 1 and 2 for country-specific appliactions. "],["part-1-basics-fundamentals.html", "Chapter 1 PART 1 - Basics &amp; Fundamentals 1.1 Programming and Remote Sensing Basics 1.2 Image and Image Collection Manipulation 1.3 Supervised Classification using Random Forest 1.4 Post-classification processing", " Chapter 1 PART 1 - Basics &amp; Fundamentals 1.1 Programming and Remote Sensing Basics 1.1.1 Remote Sensing Language A) Definition The term Remote sensing has been variously defined. Some of its early definitions include: The art or science of telling something about an object without touching it. (Fischer et al., 1976) Remote sensing is the acquisition of physical data of an object without touch or contact. (Lintz and Simonett, 1976) Remote sensing is the observation of a target by a device separated from it by some distance. (Barrett and Curtis, 1976) The term remote sensing in its broadest sense means &quot;reconnaissance at a distance.&quot; (Colwell, 1966) Thus, in the context of this training, we can define Remote Sensing as the science of acquiring information about a given target, object or phenomenon on the surface of the Earth by sensors on-board various platforms orbiting our planet. B) Building blocks of remote sensing Although the many different methods for collection, processing and interpretation of remotely sense data can vary widely, they will always have the following essential components: Figure 1.1: Basic components of a remote sensing system. Energy Source The source of the electromagnetic radiation/energy (EMR) is the first requirement of any remote sensing process. The electromagnetic spectrum is used to &quot;classify&quot; the EMR according to its wavelength: Figure 1.2: The Electromagnetic Spectrum Depending on the source of energy they are using, the different remote sensing systems can be classified as active or passive. Active sensors will produce its own source of energy for the illumination of the target. They will emit the energy toward the target being investigated and the energy reflected by this target is detected and measured by the sensor. Usually, these sensors operate in the microwave range of the electromagnetic spectrum. On the other hand, passive sensors only measures the energy that is naturally available, usually from the sun. These sensors can only be used to detect the energy being reflected during the time when the sun is illuminating the Earth. These sensors usually measure energy from the optical range (visible, near infrared, short-wave infrared and thermal infrared). Figure 1.3: Remote sensing can be classified as Passive or Active based on the source of energy. You can also think about these concepts of active and passive using a handheld photographic camera as an example: When photographing a target in the dark, the camera flash will provide the energy necessary to illuminate the target. Therefore, in that case, the camera is an active sensor. On the other hand, this same camera will be a passive sensor when you are photographing a target or object during the day, when the target being illuminated by sun light and no flash is necessary. Interaction with the target/object The most common medium in between the source and target is the atmosphere. This is where the first interaction occurs. As the EMR travel from its source to the target, it will come in contact with and interact different atmosphere constituents: aerosols, water vapor, solid particles, etc. Secondly, once the EMR makes its way through the atmospherethe to the target, it will interact with it depending on the target properties and energy wavelength. The EMR can have different types of interaction when it encounters matter; whether it is gas, solid or liquid: it can be transmitted (that is, it passes through the target), absorbed (that is, the target absorbs the energy usually increasing its temperature as a result), emitted (that is, energy is emitted from all matter at temperatures above the absolute zero of 0 Kelvins), scattered (that is, deflected in every direction) and reflected (that is, energy bounces off the target's surface and its direction is usually a function of target structure and texture). Keep in mind: All targets can show different proportions of each of these interactions. Recording of the energy by the sensor The sensor - often onboard of airplanes or satellites in space - will measure the returning EMR after it has interacted with the target and the atmosphere. This measurement is converted into a digital image with discrete values in units of digital number (DN) for each image pixel. Depending on the sensor, these resulting images will have different characteristics (or resolutions). They are: Spatial Resolution: usually known as pixel size. It refers to the sensor's ability to discriminate different objects/targets. A higher spatial resolution means a smaller pixel size which, in turn, means that smaller objects can be distinguishable as separate targets. Spectral Resolution: Different sensors will measure the EMR at specific ranges (or wavelengths), usually called bands. Thus, the spectral resolution of a sensor usually refers to the number and bandwith of these bands. Radiometric Resolution: Usually measured in bits, it refers to the sensor's ability to detect the smallest change in the spectral reflectance among different targets. For example, a 8-bit image will have 256 levels of brightness while a 16-bit image has 65,536 levels of brightness. Temporal resolution (sensors onboard satellites): is the time required for the satellite to collect two images at the same geographic location on Earth. Higher temporal resolution means less time for revisiting the same location. However, temporal resolution is usually inverselly proportional to spatial resolution: The larger the pixel size, the larger area the sensor will cover which means less time until the next revisit. Transmission, Reception, and Processing The EMR recorded by the sensor is transmitted in an electronic form to a receiving station on Earth where the data is processed and stored. Analysis and Interpretation (we are here!) This is where this training is focused on! The EMR was transformed into a digital dataset where we can use specialized instruments/hardware/software to extract information about the target observed. This extraction is often done through image processing (or digital image processing), which is the process which makes an image interpretable for a given use. There are many methods of image processing, but these are the most common ones: Image correction: The digital image recorded by the sensor on a satellite (or aircraft) may contain errors related to the geometry and brightness values of the pixels. For example, a geometrical correction, also called geo-referencing, is a procedure where the content of image will be assigned a spatial coordinate system (for example, geographical latitude and longitude). Image enhancement: This is related to modification of an image, by changing the pixel brightness values, to improve its visual aspects so that the actual analysis of images will be easier, faster and more reliable. Image classification: The overall goal of this method is to categorize all pixels in an image into themes (or land cover classes). This resulting map with its limited number of classes can be more readily and sucessfully interpreted compared to the raw image and it is often use for planning purposes. There are supervised and unsupervised methods for classification of an image: A supervised classification (human-guided) is based on the idea that a user can select sample pixels in an image that are representative of specific classes and then direct the image processing software to use these training sites as references for the classification of all other pixels in the image. These samples are selected based on the knowledge of the user. On the other hand, an unsupervised classification (computer/software-guided) is where the output classes are based on the software's ability to determine which pixels are related, using several different models and techniques. This final component of Remote Sensing (V) is achieved when we apply the extracted information to solve a particular problem. C) Spectral Signatures: A target's spectral fingerprint As mentioned before, remote sensing is based on the measurement of reflected (or emitted) radiation from different targets. Objects having different surface features reflect or absorb the sun's radiation in different ways. In order to understand and interpret the information extracted from remotely sensed data, you have to first understand the behavior of the target in respect to the electromagnetic spectrum. Each target will show a distinct reflectance pattern as a function of the wavelength, known as spectral signature (or a spectral fingerprint). This signature will directly (or indirectly) lead to the identification of a target based on its set of values for its reflectance in different spectral ranges: Figure 1.4: Typical spectral signatures of specific land cover types in the visible and infrared region of the electromagnetic spectrum (Source: http://www.seos-project.eu/) Reflectance is the ratio of the amount of light leaving a target to the amount of light striking the target. It has no units. The spectral signature of healthy green vegetation has a small reflectance in the visible portion of the electromagnetic spectrum resulting from the pigments in plant leaves. Most of the light is being used in the photosynthesis process. However, the reflectance increases dramatically in the near infrared. The spectral signature of soil is much less variable. Its behavior is affected by soil moisture, texture, surface roughness and they are less dominant than the absorbance features present in vegetation. The water's spectral signature is characterized by a high absorption at near infrared wavelengths range and beyond. Because of this absorption property, water bodies as well as features containing water can easily be detected, located and delineated with remote sensing data. These differences make it possible to identify different Earth surface features or materials by analysing their spectral reflectance patterns or spectral signatures. [add more text] References Fischer, W. A., W.R. Hemphill and A. Kover. 1976. Progress in Remote Sensing. Photogrametria, Vol. 32, pp. 33-72 Lintz, J. and D. S. Simonett. 1976. Remote Sensing of Environment. Reading, MA: Addison-Wesley. 694 pp. Barrett, E. C. and C. F. Curtis. 1976. Introduction to Environmental Remote Sensing. New York: Macmillian, 472 pp. Colwell, R. N. 1966. Uses and Limitations of Multispectral Remote Sensing. In Proceedings of the Fourth Symposium on Remote Sensing of Environment. Ann Arbor: Institute of Science and Technology, University of Michigan, pp. 71-100. 1.1.2 Google Earth Engine's Application Programming Interface (API) and Java Script Google Earth Engine is a cloud-based platform for scientific data analysis and remote sensing data processing. It provides a large catalog of ready-to-use, cloud-hosted datasets. One of Earth Engine's key features is the ability to handle computationally demading processing and analysis very fast by distributing them across a large number of servers. The ability to efficiently use the cloud-hosted datasets and computation is enabled by the Earth Engine API. An API is a way to communicate with Google Earth Engine servers. It allows you to specify what computation or command you would like to do, and then to receive the results back from the servers. The API is designed so that users do not need to worry about how the computation is distributed across a cluster of machines and the results are assembled. Users of the API simply specify what needs to be done. This greatly simplifies the code by hiding the implementation detail from the users. It also makes Earth Engine simpler for users who are not too familiar with writing code. JavaScript API and Introduction to the Code Editor The Earth Engine platform comes with a web-based Code Editor that allows you to start using the Earth Engine JavaScript API without any installation. It also provides additional functionality to display your results on a map, save your scripts, access documentation, manage tasks, and more. It has a one-click mechanism to share your code with other usersallowing for easy reproducibility and collaboration. In addition, the JavaScript API comes with a user interface library, which allows you to create charts and web-based applications with little effort. The Code Editor is an integrated development environment for the Earth Engine JavaScript API. It offers an easy way to type, debug, run, and manage code. Once you have successfully registered for a Google Earth Engine account, you can visit https://code.earthengine.google.com/ to open the Code Editor. When you first visit the Code Editor, you will see a screen such as the one shown below: Figure 1.5: Earth Engine Code Editor The Code Editor allows you to type JavaScript code and execute it. When you are first learning a new language and getting used to a new programming environment, it is customary to make a program to display your name or the words Hello World. This is a fun way to start coding that shows you how to give input to the program and how to execute it. It also shows where the program displays the output. Doing this in JavaScript is quite simple. Copy the following code into the center panel: print(&#39;Hello World&#39;); The line of code above uses the JavaScript print() function to print the text Hello World to the screen. Once you enter the code, click the Run button. The output will be displayed on the upper right-hand panel under the Console tab: Figure 1.6: Running code with GEE You now know where to type your code, how to run it, and where to look for the output. You just wrote your first Earth Engine script and may want to save it. Click the Save button to save a script: Figure 1.7: Saving a script If this is your first time using the Code Editor, you will be prompted to create a home folder. This is a folder in the cloud where all your code will be saved: Figure 1.8: Your first home folder You can pick a name of your choice, but remember that it cannot be changed and will permanently be associated with your account. Once youre home folder is created, you will be prompted to create a new repository. A repository is a like a folder where you can save your scripts. You can also share entire repositories with other users. Your account can have multiple repositories and each one can hold multiple code scripts. Start by creating a repository: Figure 1.9: Your first repository Finally, you will be able to save your script inside the newly created repository. Enter a name of your choice and click OK: Spaces are not allowed when naming scripts! Figure 1.10: Saving a script file Once the script is saved, it will appear in the script manager panel. The scripts are saved in the cloud and will always be available to you when you open the Code Editor. Figure 1.11: Your first script in your repository JavaScript Basics: Data types Javascript is the language you will use to construct and set up your commands and analysis. This section covers the basics of the Java Script sintax and some basic data structures. In the following sections, more JavaScript code will be presented. Throughout this document, code will be presented with a distinct colored font and with shaded background. As you encounter code, copy and paste it into the Code Editor and click Run. A) Variables In a programming language, variables are used to store data values. In JavaScript, a variable is defined using the var keyword followed by the name of the variable. For instance, create a var named city that contain the text string 'Monrovia'. Text strings in the code must be always in quotes. Google Earth Engine allows you to use single ' or double &quot; quotes, as long as they match in the beginning and end of the text string. We also usually end each statement on scripts with a semicolon ;, although Earth Engine's code editor does not require it. A 'i' will be shown in the line of the code where a semicolon is missing: Figure 1.12: Even though they are not a requirement, Google Earth Engine indicates that a semicolon is missing in the statement. You can hover the mouse cursor over the icon to reveal its meaning. var city = &#39;Monrovia&#39;; If you print the variable city, you will get the string stored in the variable (Monrovia) printed in the Console. print(city); When you use quotations, the variable is automatically assigned the type string. You can also assign numbers to variables. For example, create the variables population and area and assing a number as their value. When assigning numbers, you do not use commas , for thousand separators. You do, however, use . for decimals: var population = 939524; var area = 194.25; Print those variables to the Console. You can also add text to describe the variables printed in the Console. Simply add a text string within the print function along with the variable being printed separated with ,. Use the code below as example: print(&#39;Number of people in Monrovia:&#39;, population); print(area, &#39;km squared&#39;); B) Lists In the previous examples, we created variables holding a single value (text or number). JavaScript provides a data structure called a list that can be used When you want to store multiple values in a single variable. You can create lists using square brackets [] and adding multiple values separated by ,. Create a variable called listofcities, add values to it and print it to the Console: var listofcities = [&#39;Monrovia&#39;, &#39;Gbarnga&#39;,&#39;Kakata&#39;, &#39;Bensonville&#39;]; print(&#39;Largest cities in Liberia&#39;, listofcities); Looking at the output in the Console, you will see listofcities with an expander arrow () next to it. Expand the list by clicking on the arrow to show its content. You will notice that along with the items on the list, there will be a number next to each value you added. This is the index of each item. It allows you to refer to each item in the list using a numeric value that indicates its position in the list. This is useful when you want to extract a particular item from a list object. Figure 1.13: A JavaScript List. C) Objects and Dictionaries While useful to hold multiple values, lists are not appropriate to hold more structured data. JavaScript allows you to store 'key-value' information in objects or dictionaries. In this type of data structure, you can refer to a value by its key rather than its position - like in lists. You can create objects using curly braces {}. Use the code below as an example of an object: var cityData = {&#39;city&#39;: &#39;Monrovia&#39;, &#39;population&#39;: 939524, &#39;area&#39;: 194.25, &#39;coordinates&#39;: [-10.790, 6.315]}; There are a few important things about the syntax of the code above: As objects tend to hold several keys and values, it can be difficult to read the code if it is written on a continuous string. To improve readability you can use multiple lines instead: var cityData = { &#39;city&#39;: &#39;Monrovia&#39;, &#39;population&#39;: 939524, &#39;area&#39;: 194.25, &#39;coordinates&#39;: [-10.790, 6.315] }; Note how each key-value pair is on a different line. It is much easier to organize your key-value information this way. Additionally, the code can be more easily read. Second, note that the object above can holds multiple types of types of values (string and numbers) and structure (list)! If you print cityData to the Console, you can see that instead of a numeric index, each value will be identified by its key. Figure 1.14: A JavaScript Object. This key can also be used to retrieve the value of an item within an object or dictionary. If you want to retrieve a particular key from a dictionary, simply use ['key']. For example, if you want to retrieve the population value from this object and print it individually to the Console, you can use a code like this: print(cityData[&#39;population&#39;]); The same logic can be applied to lists. Always remember that lists have numeric index. Therefore, when retrieving itens from a list, use the number of its position on the list: print(listofcities[&#39;2&#39;]); D) Functions Functions are often used to group a set of operations and used to repeat the same operation with different set of parameters without having to rewrite the code for every iteration. In other words, you can call a function with different parameters to generate different outputs without changing the code. Functions are defined using function(). They often take parameters which tell the function what to do. These parameters go inside the parentheses (). Below is an example of a function named SumFunction to calculate the sum of two numbers: fistValue and secondValue. The var sum adds those two parameters and return is used to generate the output of that operation function SumFunction (firstValue, secondValue) { var sum = firstValue + secondValue; return sum; } Note that if you run the code above, nothing happens. This is just a function and it needs to be given the parameters. For example, if you need to add 37 to 584 using this function and print the result into the Console, you can use the code below: var result = SumFunction(37,584); print(result); When you call SumFunction, it will always perform the operation sum with watever two parameters you define in (). As mentioned, you can perform several operations at once using functions. For example, you can add other operations in the function above and return a list or a dictionary with the results: function MathFunction (firstValue, secondValue) { var sum = firstValue + secondValue; var sub = firstValue - secondValue; var fraction = firstValue / secondValue; return {&#39;Result Sum&#39;:sum, &#39;Result Substraction&#39;:sub, &#39;Result Division&#39;: fraction}; } Using MathFunction with a pair of parameters (firstValue and SecondValue) will return an object with the results of each operation: var results = MathFunction(500,5); print(results); Figure 1.15: Printed results of MathFunction. Earth Engine Containers and Objects So far, you learned the different data structure/types you can use within Google Earth Engine. However, if you want to do any computation with the data stored in these different types of structure, you will have to use an Earth Engine container. The ee package is used for formulating requests to Earth Engine. In other words, ee (prefix for Earth Engine) allows you to request Earth Engine servers to perform a certain computation to an variable or object. Each ee object has many different methods. You can think of methods as the many different things and computations that can be done for an specific object. In the Code Editor, you can switch to the Docs tab to see the API functions grouped by object types. Figure 1.16: List of available objects in the Docs tab of GEE API. Below are some examples to ilustrate the concept behind an Earth Engine object: A) Strings You can put strings into a ee.String object to be sent to Earth Engine. Using an object/container allows you to do manipulate them in many different ways. For example, ee.String has the: * toLowerCase() method. This method will convert any string in a ee.String object to lower case; * length() method. This method will count the characters in your string; * cat() method. This method will concatenate two strings into one; * and many others! Make sure to check the Docs tab for all the available methods for a given object. The code below uses some of the methods of ee.String as an example: var string = ee.String(&#39;HELLO EVERYONE, &#39;); print(string.toLowerCase()); print(string.length()); var string2 = ee.String(&#39;nice to meet you&#39;); print(string.cat(string2)); In your console tab you will see that the 'HELLO EVERYONE, ' string is now all in lower case. You will also see that .length() returned 16 as the number of characters for that string. Consider the code below: var string = &#39;HELLO EVERYONE&#39;; var string2 = &#39;nice to meet you&#39;; print(string.cat(string2)); If you try to use a method on a non ee object, you will get an error. Figure 1.17: Trying to use a method on non-ee.Object will return an error B) Numbers ee.Number is just another example of an object from that list. Similarly to ee.String, it will have many methods to be used with. For example, .add(), .subtract(), .divide() and .multiply () will perform these operations on ee.Number objects. Make sure to consult the Docs tab for all the different methods for each object. var value1 = ee.Number(100); var value2 = ee.Number(2); print(value1.add(value2)); print(value1.subtract(value2)); print(value1.divide(value2)); print(value1.multiply(value2)); print(value1.log10()); B) Lists You can also make a JavaScript list into an ee.List on Earth Engine server by simply casting your list into the container. There are many other useful methods to use when you have an ee.List. For instance, instead of making a JavaScript list by typing each value, you can use ee.List.sequence() to construct a list. This method will take many arguments, such as the value of the start of the list, the value at the end of the list, steps (increment) and count. Consider the examples below: Making a list with numbers from 0 to 10: print(&#39;List 1&#39;, ee.List.sequence(0,10)) Making a list from 0 to 10 with increments of 2: print(&#39;List 2&#39;, ee.List.sequence(0,10,2)) Remember: A var string = 'text' is a JavaScript string that does not allow computations. A var string = ee.String('text') is an Earth Engine object that allows computations! 1.1.3 Exploring Image and Image Collection Now that you learned the basics of JavaScript and Earth Engine objects, you have the tools at your disposal to start using the Earth Engine API to build scripts for remote sensing analysis. In this section, we explore satellite imagery, which are one of GEE's core capabilities! A) Single band image The first thing you need to know is that when working with images in Earth Engine, you will have to use an ee.Image() container. The argument provided to the constructor is the string ID of an image in the Earth Engine data catalog. (Remember to always see the Docs tab for a full list of arguments to this container). To retrieve an image ID, you can search in the Earth Engine catalog using the search tool at the top of the Code Editor: Figure 1.18: You can browse through Earth Engine's data catalog with the search tool For example, try typing 'elevation' into the search field and note that a list of rasters is returned: Figure 1.19: Typing in the search field. Google Earth Engine will show all datasets available with that criteria. Click in any of the dataset entries to see more information about that dataset. From the list above, explore the dataset 'SRTM Digital Elevation Data Version 4'. You can also find more information about this dataset on the tabs on the top right side of the dataset description screen. On the left side of the dataset description screen, you can find the Image ID, which is what we use with the ee.Image(). Alternatively, you can use the Import button on the dataset description. Using the Import button, a variable is automatically created in a special section, named 'Imports', at the top of your script. You can rename the variable by clicking on its name in the imports section. Figure 1.20: Data description Copy the image ID from the screen above and add it to a var elevationImage with the ee.Image() container as shown below: var elevationImage = ee.Image(&#39;CGIAR/SRTM90_V4&#39;); You can also retrieve the metadata about this image by using print(). In the Console, click the expander arrows to show the information. You will discover that the SRTM image has one band called 'elevation'. print(elevationImage); Figure 1.21: Image description using print() To display the image in the Map Editor, use the Map object's .addLayer() method. When you add an image to a map using Map.addLayer(), Earth Engine needs to determine how to map the values in the image band(s) to colors on the display. If a single-band image is added to a map - which is our case- by default Earth Engine displays the band in grayscale, where the minimum value is assigned to black, and the maximum value is assigned to white. If you don't specify what the minimum and maximum should be, Earth Engine will use default values. Map.addLayer(elevationImage); Do not worry! You can make it look better with custom visualization parameters! To change the way the data are stretched, you can provide another parameter to the Map.addLayer() call. Specifically, the second parameter, visParams, lets you specify the minimum and maximum values to display. One way of gauging the range of values of an image is by activating the Inspector tab and click around on the map. You will be able to see the value of that band for this dataset at that particular location: Figure 1.22: Inspector tab. Click in the image and the inspector tab will display the data for that point Click around the area of Liberia to have a sense of the range of values for this dataset. Suppose that, through further investigation, you determine that the best range of values to display elevation data in Liberia is [0,1000]. To display the data using this range, you can use a dictionary containing two keys: min and max and their respective values 0 and 1000. A third parameter for Map.addLayer() is the name of the layer that is displayed in the Layer manager. Thus your code should be looking like the one below: Map.addLayer(elevationImage, {min:0, max:1000}, &#39;Custom Visualization&#39;); Run the code and you will see something like: Figure 1.23: Custom Visualization for the SRTM data based on range of values. You can further improve your image display by using a color palette. Palettes let you set the color scheme for single-band images. A palette is a comma delimited list of color strings which are linearly interpolated between the maximum and minimum values in the visualization parameters. To display this elevation band using a color palette, add a palette property to the visParams dictionary: Map.addLayer(elevationImage, {min:0, max:1000,palette: [&#39;green&#39;, &#39;yellow&#39;, &#39;red&#39;]}, &#39;Custom Visualization - Color&#39;); Figure 1.24: Custom Visualization for the SRTM data based on range of values and color palette. In the next section, you'll learn how to display multi-band imagery. B) Multi band image To ilustrate the concepts and codes in this section, we will use a Landsat 8 image from April 2022 over Cairo, Egypt. Landsat is a set of multispectral satellites developed by the NASA (National Aeronautics and Space Administration of USA), since the early 1970s. Landsat images are very used for environmental research. These images have from 8 to 11 spectral bands (spectral resolution), with pixels of 30x30 meters (spatial resolution) and with a revisit time of 16 days (temporal resolution). Copy and paste the code below into the Code Editor: var ImageL8 = ee.Image(&#39;LANDSAT/LC08/C02/T1_L2/LC08_176039_20220406&#39;); As learned before, the code above is casting a Landsat 8 image into an ee.Image container using its ID into a var called 'ImageL8'. By clicking Run, Earth Engine will retrieve this image from its Landsat image catalog. You will not yet see any output. As you learned, you can retrieve additional information about this image by using print(). print(ImageL8) In the Console panel, you may need to click the expander arrows to show the information. You should be able to read that this image consists of 19 different bands (index 0 to 18). Each band will have 4 properties: name, data type (as in if the value is an integer, float, ect.), projection and dimensions (in rows and columns of pixels). For this example, simply note the first property of the first band: Figure 1.25: Landsat 8 image metadata. A satellite sensor like Landsat 8 measures the EMR in different portions of the electromagnetic spectrum. Six out of seven first bands in our image (&quot;SR_B2&quot; through &quot;SR_B7&quot;) contain measurements for six different portions of the spectrum: Figure 1.26: Landsat 8 bands. Source: https://www.usgs.gov/faqs/what-are-band-designations-landsat-satellites Now, let's visualize ImageL8. First, make sure your map is somewhere near Cairo, Egypt. To do this, click and drag the map towards Cairo, Egypt. (You can also jump there by typing Cairo into the Search panel at the top of the Code Editor). Add ImageL8 to the Map as a layer using the code below: Map.addLayer(ImageL8); You probably see a gray image with not a lot of details. From the previous section you learned that Map.addLayer take many parameters. One of them is the visParams, which lets you specify the minimum and maximum values to display. Similarly to the previous section, you can use the Inspector tab to investigate the range of values for each location for each of ImageL8's band. Now, specify a range of values to be displayed. Follow the code example below: Map.addLayer(ImageL8, {min:8000, max:20000}, &#39;My First Image&#39;); You should see something like this: Figure 1.27: Landsat 8 image in false color . By default, if you do not specify the bands to be displayed within the visParams, Earth Engine will display the first three bands, each band on each RGB (red, green, blue) channel of your monitor screen. In other words, Earth Engine is displaying 'SR_B1' (Landsat 8's coastal aerosol band) in the red channel, 'SR_B2' (Landsat 8's blue band) in the green channel and 'SR_B3 (Landsat 8's green band). This is a case of a false color display, when spectral bands do not match the RGB channels in your screen. False color displays have many advantages and they will be explored later in the tutorial. For a real color display, you will need to define the bands within visParams to match the RGB channels: Map.addLayer(ImageL8, {bands: [&#39;SR_B4&#39;, &#39;SR_B3&#39;, &#39;SR_B2&#39;], min:8000, max:20000}, &#39;Real color&#39;); In the code above, visParams contains a list (bands) with the names of the ImageL8's bands to be displayed in the RGB channels of your screen. This band set up tells Earth Engine to display the red band in the red channel, the green band in the green channel and the blue band in blue channel. We call this composition a RGB 432 or Real color composition. You should see an image like this when running the code above: Figure 1.28: Landsat 8 image displayed with a real color composition. You can easily change these values and settings within visParams by using the Layer setting option in the Map editor. You can find it by hovering your cursor over the Layer List (upper right corner of the Map editor, next to the Map and Satellite buttons) and clicking the gear icon of the layer: Figure 1.29: Layer settings window. Once you chose your settings, click Apply. Earth Engine will automatically apply the new settings and display the layer. Try using different band combinations and range values to see how the image changes. Different color compositions will highlight different features in the image based on their spectral signatures! C) Image collections An image collection refers to a set of Earth Engine images. For example, the collection of all Landsat 8 images! In this case, you will use ee.ImageCollection() instead of ee.Image() to retrieve a particular image collection . Like the SRTM image or the Landsat image you have been working with, image collections also have an ID. Similarly to the single images, you can discover the ID of an image collection by searching the Earth Engine data catalog from the Code Editor. Start by loading the Landsat 8 image collection into a var called 'collection using the ee.ImageCollection container. Then try to print this collection to the Console: var collection = ee.ImageCollection(&#39;LANDSAT/LC08/C02/T1_L2&#39;); print(collection); You will notice that you will get an error when trying to print collection to the Console: Figure 1.30: Trying to print an image collection without any filter. It's worth noting that this collection represents every Landsat 8 scene collected, all over the Earth. Thus, Earth Engine is not able to print the information for every scene all over the globe since 2013 (when Landsat 8 started collecting data). In this case we need to filter this collection. Exploring the Docs tab of the Code Editor to learn more about ee.ImageCollection, you will notice the methods filterBounds() and filterDate(). These are shortcut methods on a bigger filter() method. In this case, filterBounds() filters a collection by intersection with geometry (or a location) while filterDate() filters a collection by a date range, expressed as strings. Location Filter: To filter collection to images that cover a particular location, first define your area of interest with the geometry drawing tools. To create geometries, use the geometry drawing tools in the upper left corner of the map display. Figure 1.31: The Geometry tools. For drawing points, use the place mark icon, for drawing lines, use the line icon, for drawing polygons, use the polygon icon, for drawing rectangles, use the rectangle icon. Using any of the drawing tools will automatically create a new geometry layer and add an import for that layer to the Imports section (top of the script). Once you finish drawing the geometry, click Exit. To rename the geometries that are imported to your script, click the settings icon next to it (or rename it directly in the Imports section).The geometry layer settings tool will be displayed in a dialog where you can change the geometry name. Use the point mark and create a point geometry named 'aoi' (area of interest) in a location of your interest. For this example, we will use a point mark location over Cairo, Egypt. Use the figure below as a guide: 1) Click on the point mark geometry; 2) Place it in your location of interest, and; 3) rename it 'aoi'. Figure 1.32: Creating a point geometry. Date filter: Now that you have a location, create two variables startDate and endDate containing a data range expressed as strings. Use a date range of your choice. Dates are expressed as 'YYYY-MM-DD' in Earth Engine. For this example, we will use the first four months of 2022: var startDate = &#39;2022-01-01&#39;; var endDate = &#39;2022-04-30&#39;; Now that you have both filters for location and date range, you are ready to filter your image collection using filterBounds() and filterDates(): var collectionFiltered = collection.filterBounds(aoi).filterDate(startDate,endDate); print(collectionFiltered); When printing collectionFiltered to the Console you will notice that now Earth Engine was able to retrieve the information of every Landsat scene based on your filters. Using the date range and the location filters from this example, there are the 13 images in the collectionFiltered for Cairo from January to April, 2022: Figure 1.33: Filtered collection. If you want to retrieve a particular image from this collection you can simply use ee.Image() and the image ID printed in the Console tab! 1.2 Image and Image Collection Manipulation Now that you know how to load and display an image, it's time to apply a computation to it. In the following sections you will learn some examples of computations for a single-band image and multi-band image (band math and vegetation index calculation) and for an image collection. 1.2.1 Image Math Single-band image Considering our previous example, we will use the SRTM single-band image elevationImage and create a 'slope' image. Briefly, you calculate the slope by dividing the difference between the elevations of two points by the distance between them, then multiply the quotient by 100. The difference in elevation between points is called the rise. The distance between the points is called the run. Thus, percent slope equals (rise / run) x 100. Intuitively, you can think of doing this calculation manually by applying .divide() and .multiply(). However, as you will learn with this material, Google Earth Engine has specific methods within ee objects to perform computations such as slope, for example. Simply, you can create an the 'slope' image with the slope method of the ee.Terrain package: var slope = ee.Terrain.slope(elevationImage); Note that elevationImage was provided as an argument to the slope method! Add slope to the map and find Mount Nimba, in Liberia. Use a min and max values to reflect a resonable range of % slope. Map.addLayer(slope, {min: 0, max :50}, &#39;Slope&#39;); Explore the slope image around Mount Nimba, in Liberia. It should look like the figure below: Figure 1.34: Slope image calculated with the .slope() method of the ee.Terrain package. Mount Nimba in Liberia is shown in white. As mentioned before, there are also methods in the ee.Image container that can be invoked on an image object. We call it band math when you do mathematical operations with image bands. Still considering the previous example, suppose you are interested in further processing the slope image into an aspect image and then perform some trigonometric operations on it. Aspect, in this case, refers to the orientation of a slope, measured clockwise in degrees, from 0 to 360. Similarly to .slope(), .aspect() is also an method from ee.Terrain. var aspect = ee.Terrain.aspect(elevationImage); Now, convert the image aspect into radians then calculate its sin: var sinImage = aspect.divide(180).multiply(Math.PI).sin() It is worth noting that the code above chained multiple methods. This way, you can perform complex mathematical operations. In other words, the code above is simply dividing the aspect by 180 (using the .divide() method), multiplying (with .multiply()) the result of that by  (that can be retrieved using Math.PI), and finally taking the sin (with .sin()). The result should look something like the figure below: Figure 1.35: Aspect image calculated with the .aspect() method of the ee.Terrain package. The resulting SIN image from aspect reveals Mount Nimba in greater detail. Band math with multi-band image As mentioned previously, you can do mathematical operations with bands from a given image. One of the most common examples of band math with remote sensing imagery is the calculation of spectral indices. A spectral index is a mathematical equation that is applied on the various spectral bands of an image per pixel, with the objective of highlighting pixels showing the relative abundance or lack of the feature of interest. There are several categories of spectral indices that have been developed, using a variety of spectral bands to highlight different phenomena, such as water, snow, soil and vegetation. For example, Vegetation Indices (VIs) are combinations of surface reflectance at two or more wavelengths designed to highlight a particular property of vegetation. A) NDVI The most used index in this category is the Normalized Difference Vegetation Index (NDVI), which provides an indication of abundance of live green vegetation (or abundance of chlorophyll). The pigment in plant leaves, chlorophyll, strongly absorbs visible light (from 0.4 to 0.7 µm) for use in photosynthesis. The cell structure of the leaves, on the other hand, strongly reflects near-infrared light (from 0.7 to 1.1 µm) (See Figure 1.4). Thus, NDVI is calculated by comparing the different reflectance values of the red and near-infrared bands (normalized such that the minimum value is -1.0, and the maximum is +1.0). Bringing this concept to the context of a Landsat 8 image, we can produce an NDVI image by computing the normalized difference between its bands 5 (near-infrared) and 4 (red) (See Figure 1.26). For this example, we will use ImageL8 created in the previous sections to create a NDVI image. We can use the method .select() on a ee.Image object to select any given band. The .select() method will take a string as an argument for the band name you want to select. Then we can use the mathematical operators to perform a normalized difference using these bands: var nirBand = ImageL8.select(&#39;SR_B5&#39;); var redBand = ImageL8.select(&#39;SR_B4&#39;); var NDVI = nirBand.subtract(redBand).divide(nirBand.add(redBand)); There is another way of doing the same calculation from the code above. As we seen previously, Earth Engine usually have methods for widely used operations and computations with remote sensing data. In this case, the normalized difference operation is available as a shortcut method .normalizedDifference() for a ee.Image() object. It take as an argument a list with the names of the bands you wish to calculate the normalized difference with. Thus, NDVI can be rewritten as: var NDVI = ImageL8.normalizedDifference([&#39;SR_B5&#39;, &#39;SR_B4&#39;]) You can then visualize this NDVI image by adding it to the Map Editor. Add it to the map and use the Inspector tab to investigate the range of values of NDVI around Cairo: Map.addLayer(NDVI, {min: 0, max: 0.3, palette: [&#39;a6611a&#39;, &#39;f5f5f5&#39;, &#39;4dac26&#39;]}, &#39;NDVI Image&#39;); You should see something like this: Figure 1.36: NDVI Image highlighting the agricultural area in the Nile Delta. With the visualization parameters provided above, higher values of NDVI (that is, higher content of chlorophyll in live green leaves) is shown in green. A) EVI EVI, or Enhanced Vegetation Index, is similar to NDVI and can be used to quantify vegetation greenness. However, EVI corrects for some atmospheric conditions and canopy background noise and is more sensitive in areas with dense vegetation where NDVI usually saturates. It incorporates an L value to adjust for canopy background, C values as coefficients for atmospheric resistance, and values from the blue band (B): EVI = 2.5 * ((Near-infrared - Red) / (Near-infrared + C1 * Red  C2 * Blue + L)), where C1, C2 and L are usually 6, 7.5 and 1, respectively. You can see that an expression like this can become hard to be expressed using mathematical operators such as the ones used to calculate a normalized difference. To implement more complex mathematical expressions, consider using the .expression() method for a ee.Image object. the first argument to .expression() is the textual representation (string) of the math operation, the second argument is a dictionary where the keys are variable names used in the expression and the values are the image bands to be used in the operation. Using the same ImageL8 as an example, EVI would be defined as: var EVI = ImageL8.expression( &#39;2.5 * ((NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))&#39;, { &#39;NIR&#39;: ImageL8.select(&#39;SR_B5&#39;), &#39;RED&#39;: ImageL8.select(&#39;SR_B4&#39;), &#39;BLUE&#39;: ImageL8.select(&#39;SR_B2&#39;) }); The code above is able to regonize operators (+, -, *, /, %, **: Add, Subtract, Multiply, Divide, Modulus, Exponent) and the numbers; everything else is defined as keys in the dictionary. Can you think of how you can calculate the NDVI using the code structure above? 1.2.2 Compositing and Mosaicking Compositing is an example of a computation that can be done to a image collection. In general, compositing refers to the process of combining spatially overlapping images into a single image based on an aggregation function while mosaicking usually refers to the assembly of these images to produce a spatially contiguous one. In Earth Engine, when you composite two scenes that do not overlap completely, the resulting composite will be a composited mosaic: Figure 1.37: Compositing images in Earth Engine. When applying a compositing function to scenes that do not overlap completely (B), the resulting composite will be a spatially contiguous image. Thefore, in the context of Earth Engine these terms are used interchangeably. We illustrate these concepts with the examples below. Example 1: Compositing the same scene (100% overlap) In this example, we will use collectionFiltered (a Landsat 8 image collection filtered for Cairo from January to April 2022. See 1.33). There are several aggregation functions that can be used to composite an image collection, most notably .max() for a maximum value composite and .mendian() for a median value composite. All these functions will aggregate the values on a per-pixel, per-band basis. In other words, .max() will select the maximum value of a given pixel for each image band to create the final composite while .median() will calculate the median value of each band for this pixel. var compositeMax = collectionFiltered.max(); var compositeMedian = collectionFiltered.median(); If you print these composites to the Console you will notice that they are a 19-band single image and no longer a collection with 13 images with 19 bands each! Add these composites to the Map Editor to investigate their differences: Map.addLayer(compositeMax, {bands: [&#39;SR_B4&#39;, &#39;SR_B3&#39;, &#39;SR_B2&#39;], min:8000, max:30000}, &#39;Composite Max&#39;); Map.addLayer(compositeMedian, {bands: [&#39;SR_B4&#39;, &#39;SR_B3&#39;, &#39;SR_B2&#39;], min:8000, max:30000}, &#39;Composite Median&#39;); Question: What is the most evident difference between these composites? Answer: The max value composite is mostly covered by clouds. Clouds and snow are very reflective and will always have high reflectance values for the regions of the electromagnetic spectrum covered by the Landsat spectral bands. Therefore, a maximum value composite will likely highlight these features - which is not desireable in most cases. Figure 1.38: Maximum and median value composites. Clouds are evident in the maximum value composite due to their high reflectance values. Although there are different methods for compositing, the median composite method is the state of the art in Google Earth Engine and has been applied to a multitude of studies using multitemporal remote sensing data. Example 2: Compositing (mosaicking) different scenes Consider the need to composite (mosaic) four different Landsat scenes at different locations. For that we will edit the location parameter from .filterBounds() to include extra Landsat scenes from different locations. Use the information from figures 1.31 and 1.32 to create a geometry covering a larger area. Remember that .filterBounds() will filter an image collection to all the scenes that intercept the boundaries of a geometry, whether it is a line, point mark or polygon. Rename this as 'aoi2'. For this example, we will use the polygon tool to draw a large polygon over a large area in Egypt. Figure 1.39: Area of interest (aoi) for filtering image collection. We will use this geometry to filter collection to this new spatial filter aoi2: var collectionFiltered2 = collection.filterBounds(aoi2).filterDate(startDate,endDate); print(collectionFiltered2); Note that if you print collectionFiltered2 to the Console you will notice that it includes more images than when we filtered for a single location using a point geometry. Using the same compositing functions described earlier in this section, we will composite this new collection to two new composites. Also, we will use the .mosaic() method as a comparison. This method composites overlapping images according to their order in the collection (last on top): var compositeMax2 = collectionFiltered2.max(); var compositeMedian2 = collectionFiltered2.median(); var mosaic = collectionFiltered2.mosaic(); Map.addLayer(compositeMax2, {bands: [&#39;SR_B4&#39;, &#39;SR_B3&#39;, &#39;SR_B2&#39;], min:8000, max:30000}, &#39;Composite Max 2&#39;); Map.addLayer(compositeMedian2, {bands: [&#39;SR_B4&#39;, &#39;SR_B3&#39;, &#39;SR_B2&#39;], min:8000, max:30000}, &#39;Composite Median 2&#39;); Map.addLayer(mosaic, {bands: [&#39;SR_B4&#39;, &#39;SR_B3&#39;, &#39;SR_B2&#39;], min:8000, max:30000}, &#39;Mosaic&#39;); Figure 1.40: Composites of an image collection filtered by the bounds of aoi2. Whether you are compositing using an aggregation function or mosaicking with .mosaic() the result will be similar in respect to the output type (i.e. an ee.Image object) and spatial extent. Thus, explaining why compositing and mosaicking are used interchangeably in Earth Engine. Suppose you are only interested in the composite within the bounds of your area of interest. The method .clip() of an ee.Image() allows you to clip (or cut) any image to the shape of a geometry. We will use compositeMedian2 and aoi2 as an example: var compositeClipped = compositeMedian2.clip(aoi2); Map.addLayer(compositeClipped, {bands: [&#39;SR_B4&#39;, &#39;SR_B3&#39;, &#39;SR_B2&#39;], min:8000, max:30000}, &#39;Composite Median 2 clipped&#39;); Figure 1.41: Median composite clipped by the extent of aoi2. You can only clip an ee.Image() object, never an image collection! `` PART 2 - Advanced Google Earth Engine Advanced Image Manipulation: Pre-classification In this section we will provide functions and lines of code to assist the user to prepare, process and analyze Landsat 8 data for classification purposes. Cloud and cloud shadow masking This sub-section demonstrates a way of masking clouds and cloud shadow pixels from Landsat 8 Surface Reflectance Collection 2 data based on file metadata. You may have noticed when exploring Landsat images in the Console tab that these images have a band called QA_PIXEL or Quality Assessment band. Briefly, this band contains values that represent bit-packed combinations of surface, atmospheric, and sensor conditions that can affect the overall usefulness of a given pixel. One of the many bits represented in this band is cloud (bit 3) and cloud shadow (bit 4): Figure 1.42: Landsat's Quality Assessment (QA) band. In essence, these values indicate which pixels might be affected by surface conditions such as cloud contamination. Therefore, this band can be used to construct filters to mask (or remove) pixels flagged as 'affected' by that condition. Here, we will provide a function for masking clouds and cloud shadow from Landsat 8 images based on the information stored in the Quality Assessment band. This function was created based on the documentation available for Landsat 8. Values for pixel bit and pixel band were found here. The cloud masking function is as follows: function maskClouds(image) { var cloudShadowBitMask = ee.Number(2).pow(3).int(); var cloudsBitMask = ee.Number(2).pow(4).int(); var QA = image.select(&#39;QA_PIXEL&#39;); var mask = QA.bitwiseAnd(cloudShadowBitMask).eq(0) .and(QA.bitwiseAnd(cloudsBitMask).eq(0)); return image.updateMask(mask).divide(100000).select(&quot;SR_B[0-9]*&quot;).copyProperties(image, [&quot;system:time_start&quot;]); } Do not worry about the new methods within this function. What you need to know is that this function was constructed in a way that we are selecting the QA_PIXEL band from the Landsat image and creating a mask where only pixels flagged to 0 (indicating clear conditions) for bits 3 and 4 will be included; The function will return image - in this case the Landsat 8 image - masked for pixels that are not flagged 0 for bits 3 and 4. The .divide() was used simply to scale the band values to [0,1] and .select() to only select the spectral bands from Landsat (SR_B1-9). These steps are not necessary for the cloud masking to work! However, we can add these extra steps to further improve our image collection. Now, to apply a function to every image in an image collection, we use the .map() method from the ee.ImageCollection() object. You can refer to the Docs tab for more information on the methods available for ee.ImageCollection(). The only argument to map() is a function which takes one single parameter: an ee.Image(). Using .map(maskClouds) over an image collection will result in a collection where every image is masked for clouds and cloud shadows. To illustrate this, we will apply maskClouds over collection (created in the previous chapter) along with the same temporal and spatial filters to recreate collectionFiltered. Feel free to use your previous chapter's script and simply add .map() to the collections using the code below as an example: var collection = ee.ImageCollection(&#39;LANDSAT/LC08/C02/T1_L2&#39;); var startDate = &#39;2022-01-01&#39;; var endDate = &#39;2022-04-30&#39;; var collectionFilteredMasked = collection.filterBounds(aoi) .filterDate(startDate,endDate) .map(maskClouds); Now, we will create a max value composite and compare it to the previous composite created with an image collection that was not masked for clouds and cloud shadows: var compositeMax = collectionFilteredMasked.max(); Map.addLayer(compositeMax, {bands: [&#39;SR_B4&#39;, &#39;SR_B3&#39;, &#39;SR_B2&#39;], min:0, max:0.25}, &#39;Composite Max Value&#39;); Figure 1.43: A maximum value composite created with an image collection masked by clouds compared to a composite created using an unmasked collection. As mentioned in the previous chapter, a maximum value composite will likely highlight clouds as they are very reflective. However, by removing the cloud pixels from the images based on the Quality Assessment band information, the resulting maximum value composite greatly improves. 1.2.3 Spectral indices In the previous sub-section, we presented a function to mask cloud and cloud shadows from Landsat image collections. A function can also be created to perform band math to every image in an image collection. Building on the concepts of vegetation indices presented on the previous chapter, we will create a function that will calculate several vegetation indices for Landsat 8 images. You will recognize the NDVI and EVI indices from the previous chapters. This function includes other commonly used spectral indices to highlight features like open water, moisture, vegetation and soil: NDVI - Normalized Difference Vegetation Index; NBR - Normalized Burn Ratio; NDMI - Normalized Difference Mangrove Index; MNDWI - Modified Normalized Difference Water Index; SR - Simple Ratio; BI - Bare soil Index; GCVI - Green Chlorophyll Vegetation Index; EVI - Enhanced Vegetation Index, and; MSAVI - Modified Soil-Adjusted Vegetation Index. function addIndices(image) { var ndvi = image.normalizedDifference([&#39;SR_B5&#39;,&#39;SR_B4&#39;]).rename(&#39;NDVI&#39;); var nbr = image.normalizedDifference([&#39;SR_B5&#39;,&#39;SR_B7&#39;]).rename(&#39;NBR&#39;); var ndmi = image.normalizedDifference([&#39;SR_B7&#39;,&#39;SR_B3&#39;]).rename(&#39;NDMI&#39;); var mndwi = image.normalizedDifference([&#39;SR_B3&#39;,&#39;SR_B6&#39;]).rename(&#39;MNDWI&#39;); var sr = image.select(&#39;SR_B5&#39;).divide(image.select(&#39;SR_B4&#39;)).rename(&#39;SR&#39;); var bare = image.normalizedDifference([&#39;SR_B6&#39;,&#39;SR_B7&#39;]).rename(&#39;BI&#39;); var gcvi = image.expression(&#39;(NIR/GREEN)-1&#39;,{ &#39;NIR&#39;:image.select(&#39;SR_B5&#39;), &#39;GREEN&#39;:image.select(&#39;SR_B3&#39;) }).rename(&#39;GCVI&#39;); var evi = image.expression( &#39;2.5 * ((NIR-RED) / (NIR + 6 * RED - 7.5* SR_BLUE +1))&#39;, { &#39;NIR&#39;:image.select(&#39;SR_B5&#39;), &#39;RED&#39;:image.select(&#39;SR_B4&#39;), &#39;SR_BLUE&#39;:image.select(&#39;SR_B2&#39;) }).rename(&#39;EVI&#39;); var msavi = image.expression( &#39;(2 * NIR + 1 - sqrt(pow((2 * NIR + 1), 2) - 8 * (NIR - RED)) ) / 2&#39;, { &#39;NIR&#39;: image.select(&#39;SR_B5&#39;), &#39;RED&#39;: image.select(&#39;SR_B4&#39;)} ).rename(&#39;MSAVI&#39;); return image .addBands(ndvi) .addBands(nbr) .addBands(ndmi) .addBands(mndwi) .addBands(sr) .addBands(evi) .addBands(msavi) .addBands(gcvi) .addBands(bare); } The .addBands() method is used to include an ee.Image() as a band to an existing image. Therefore, this function will calculate each index and include them as extra bands to every image in the image collection. As in the previous sub-section, we can use .map() to map this function over collection: var collectionFilteredwithIndex = collection.filterBounds(aoi) .filterDate(startDate,endDate) .map(maskClouds) .map (addIndices); Then we will create median composite based on this new collection. We will add it to the Map Editor and will use the Inspector tab to explore its band values at a given location: var compositeMedian = collectionFilteredwithIndex.median(); Map.addLayer(compositeMedian, {bands: [&#39;SR_B4&#39;, &#39;SR_B3&#39;, &#39;SR_B2&#39;], min:0, max:0.25}, &#39;Composite Median&#39;); You can toggle the data view from chart to values with the chart/value view button: Figure 1.44: A median composite created from the collectionFilteredwithIndex image collection. The function addIndices was used to calculate each index and add them as separate bands to every image in the collection. In this example, the spectral bands and spectral index values for a pixel at an arbitrary location within compositeMedian. You can toggle between the chart view and value view using the chart button (red circle). 1.3 Supervised Classification using Random Forest As seen in Part 1, Interpretation and Analysis is one of the building blocks of any remote sensing system (see 1.1). Image classification is one of the many methods of image processing and it is the sole focus of this training material. So far, you have learned the basics of Java Script and Google Earth Engine and some pre-processing steps to perform an image classification using the Random Forest (RF) Classifier. Briefly, classifier is an ensemble of classification trees, where each tree contributes with a single vote for the assignment of the most frequent class to the input data. Different from Decision Trees, which use the best predictive variables at the split, RF uses a random subset of the predictive variables. To illustrate this concept, we will use a rather simplistic example: Suppose you were given this list of attributes from three edible fruits: Figure 1.45: Simplistic list of predictive variables from three types of fruits. A random forest classifier can be created (or trained) using the sample above as a training sample set: Figure 1.46: A Machine Learning Random Forest Classifier with 3 trees trained with the sample set above. Note that algorithm was able to 'learn' about each fruit separate them based on their attributes. You can test this classifier and its accuracy using a testing sample set. A testing set is usually a subset of the training set that will be used only for testing the performance of the trained classifier. These testing samples will be run through each tree of the ensemble and the accuracy of the classifier will be based on whether or not it was able to classify that sample correctly: Figure 1.47: Testing a trained Random Forest Classifier. A testing sample is used to assess whether or not the classifier is able to correctly classify them. In this example, this sample was correctly classified by 2 out of the 3 trees in the ensenble. You can then classify unlabeled samples using the trained classfier. The final classification of a sample is based upon the majority of the votes of the trees in the ensemble: Figure 1.48: The trained RF classifier is used to classify an unlabeled fruit sample. In this case, the majority of the votes (2 out of 3) was BANANA. Therefore, the final classification of this sample is BANANA. Even though this is a rather simplistic example, it can be easily translated into a land cover classification context: Instead of fruits, each unit being classified is a Landsat image pixel. Each pixel from each land cover class will contain different spectral attributes. These spectral attributes will be used at each branch of the trees in the ensemble. We can then used a trained Random Forest classifier to label hundreds of thousands Landsat pixels into different land cover classes. The RF is one of the most used and robust classifiers and it is fully implemented in GEE. In the following examples we will present the steps for land cover classification using the RF classifier and present some basic analysis that can be done using the classification output. 1.3.1 Example 1: Land cover classification of Greater Cairo and Giza area, Egypt - Year 2022 The general steps for an image classification process with Landsat image is: To use a cloud-masking function to mask clouds on Landsat 8 Imagery; To calculate spectral indices that will be used as predictors for the Random Forest; To produce a cloud-free composite mosaic using the median reducer, and; To select training samples; To classify the cloud-free composite mosaic of Landsat 8 scenes using Random Forest. In this example we will classify the median composite (compositeMedian) created in the previous section. You can find the code for creating it here. You have the option to classify the entire scene or clip it to an area of interest. You can create a geometry AreaOfInterest and clip your composite using .clip(AreaOfInterest). You can also customize the composite visualization parameters by clicking on the settings icon (gear ) next to the layer name (in this example: 'Composite Median') So far, we have covered the steps a, b and c. Training sample selection For this example, lets classify the 'composite' into four classes: water, agricultural land, sand and bare areas and urbanization. The first step is to create the training samples set to use into the Random Forest Model. Step 1 - In the Geometry Imports, click +new layer and make four sets of geometries, each set will represent samples from the classes 'water', 'cropland', 'sand', and 'urban'. Figure 1.49: Geometry sets to hold samples for each of the four classes. Step 2 - For each geometry in the list, click on the settings icon: name them accordingly using the 'Name' box, choose a color for it using the color picker and import each geometry as FeatureCollection. Add a property called landcover by clicking on the + Property and set a consecutive integer starting from 0 or 1 for each of the classes. You should achieve something similar to this: Figure 1.50: Geometries used to create the training sample sets. Your Geometry Imports should look like this: Figure 1.51: Geometry Imports after creating geometry sets to hold training samples Start selecting samples by clicking on the Water geometry in the Geometry Imports. Choose the point drawing tool and place some points along the River Nile: Take advantage of the high resolution imagery to help you select samples for each class. You can toggle in between map and Google Earth imagery by using the buttons Map and Satellite in the upper right corner of the Map Editor. You can also toggle the Landsat composite ON and OFF by using the layer manager. Instead of single points (i.e pixels), we can also use polygons containing a variable number of relatively homogenous pixels of a given land cover class. Switch to the polygon drawing tool and draw a few polygons over the River Nile: Figure 1.52: Sample points and polygons for the Water class Once you finish selecting samples, click the Exit button on the polygon editor dialogue box . Repeat the process for each of the other class. Make sure you select samples that are representative of each land cover class by selecting points and polygons of homogenous pixels. Figure 1.53: Example of sample points and polygons for Water (blue), Cropland (yellow), Urban (red) and Sand (pink). Each pixel within the polygons will be used as training inputs for the RF classifier. After selecting the samples, we will merge all the geometries together into an object var classes using the .merge method of the ee.FeatureCollection() object: var classes = Water.merge(Cropland) .merge(Sand) .merge(Urban); Code Checkpoint Sample sets In this section, we will create the training (and testing) sample sets to be used in the classification with Random Forest. First, we will select the predictors to assign to each sample point in the sample sets. For this example, we will create a list 'bands' with the names of three spectral bands ('SR_B4','SR_B5','SR_B6') and the spectral indices ('NDVI','NBR','MNDWI','SR','GCVI' and 'MSAVI'). var bands = [&#39;SR_B4&#39;,&#39;SR_B5&#39;,&#39;SR_B6&#39;,&#39;NDVI&#39;,&#39;NBR&#39;,&#39;MNDWI&#39;,&#39;SR&#39;,&#39;GCVI&#39;,&#39;MSAVI&#39;]; Next, we will sample the Landsat pixels by overlaying the geometries with the composite using .sampleRegions(). The main arguments of this method is the image to sample (in this case, compositeMedian), the regions to sample over (in this case, classes) and the list of properties to copy from each geometry (in this case landcover): var samples = compositeMedian.select(bands).sampleRegions({ collection: classes, properties: [&#39;landcover&#39;], scale: 30 }).randomColumn(&#39;random&#39;); In the samples object we have just created, each sample will include a column with the values from the list bands inherited from the compositeMedian and a column with their respective class label. Optionally, you can perform an accuracy assessment of the classifier by taking advantage of the identifiers assigned to the samples by the .randomColumn('random') within samples. This method adds a column to the feature collection populated with random numbers in the range of 0 to 1. For this example, we will randomly partition the sample set into training (80% of the samples) and testing (20% of the samples) samples by filtering the samples by its random number column using the lower (.lt) and greater than or equal (.gte) filters: var split = 0.8; var training = samples.filter(ee.Filter.lt(&#39;random&#39;, split)); var testing = samples.filter(ee.Filter.gte(&#39;random&#39;, split)); For your information, you can inspect the size of a ee.FeatureCollection using the .aggregate_count() method. This method only takes a property of the feature collection being counted as its argument. In this case, we have a property called 'landcover' in our feature collection. .aggregate_count() It is a useful tool to extract the number of features on your sample set: print(&#39;Samples n =&#39;, samples.aggregate_count(&#39;landcover&#39;)); print(&#39;Training n =&#39;, training.aggregate_count(&#39;landcover&#39;)); print(&#39;Testing n =&#39;, testing.aggregate_count(&#39;landcover&#39;)); If a feature collection do not have a property defined, you can still count its feature by using '.all' as an argument for .aggregate_count(). Using the split value above, roughtly 80% of the features in samples will be training and 20% will be in testing: Figure 1.54: Sample sets size for this example. Note that the number will vary based on the number of geometries (polygons and points) and the sample split value. Classification Next, we will train a Random Forest classifier using the training sample set training. The Random Forest classifier ee.Classifier.smileRandomForest has several user-defined parameters. However, two of them are the most usually defined: the number of trees in the ensemble (a.k.a forest) and the number of predictors to randomly tested at each tree. Predictors, in this case, are the spectral bands and spectral indices associated to each training sample. If unspecified, it uses the square root of the number of predictors. Higher number of trees and number of predictors tested at each node do not mean better performance and overall accuracy. For this example, we will use 200 trees and 8 randomly selected predictors to use in each tree and train Random Forest model called 'classifier' using the .train() method on the .ee.Classifier.smileRandomForest(): var classifier = ee.Classifier.smileRandomForest(100,5).train({ features: training, classProperty: &#39;landcover&#39;, inputProperties: bands }); In the features argument of the code above, make sure to select the predictors you want to use from the sample set (in this example, training) to train the model. In this case we are providing all of them. However, if you want to select a particular group of variables, you can use .select(['variableName1','variableName2',..]). Also, you must include the class property (classProperty) in which the class label is stored (in this example 'landcover'). You can test the accuracy of classifier by classifying the testing samples using .classify() method. Then, you can compute a 2D error matrix for the classified testing samples using .errorMatrix(). This method will output an ee.ConfusionMatrix() object by comparing the two columns of the classified testing samples: one containing the actual labels (in our case, the property called 'landcover'), and one containing predicted values by the classifier (which defaults to 'classification'). Finally, you can use .accuracy() method of the ee.ConfusionMatrix() object to compute the overall accuracy of the classifier: var validation = testing.classify(classifier); var testAccuracy = validation.errorMatrix(&#39;landcover&#39;, &#39;classification&#39;); print(&#39;Validation error matrix RF: &#39;, testAccuracy); print(&#39;Validation overall accuracy RF in %: &#39;, testAccuracy.accuracy().multiply(100)); As explained earlier, the trained classifier can be used to classify the compositeMedian. Google Earth Engine will run each pixel through each of the 200 trees in the classifier and assing it the label which had the majority of votes among all the tree: var classification = compositeMedian.select(bands).classify(classifier); For visualization of the resulting classification output, we will create a color palette object paletteMAP with a list of colors for each class of the map. The order of the colors in the palette will follow the order of the classes: var paletteMAP = [ &#39;#0040ff&#39;, // Water (Class value 0) &#39;#00ab0c&#39;, // Croplands / Cultivated Areas (Class value 1) &#39;#fbf2ad&#39;, // Sand and bare areas (Class value 2) &#39;#878587&#39; // Built-up and Urban Areas (Class value 3) ]; Finally, we will add classification to the map using Map.addLayer(): use min: 0 (first class), max: 3 (last class), add the color palette paletteMAP and a name for the layer: Map.addLayer (classification, {min: 0, max: 3, palette:paletteMAP}, &#39;Classification&#39;); After several seconds, you should see something similar to the figure below: Figure 1.55: Random Forest classification output for 2022. Code Checkpoint As an optional step, you can export your classification output as a Google Earth Engine asset. You can save geospatial datasets and analysis outputs into your Google Earth Engine account through the Assets tab and the left side of the code Editor. Figure 1.56: Assets manager. Your Google Earth Engine account will be able to host around 200GB worth of assets. Externally imported assets as well as datasets exported from Earth Engine scripts will be found here. You can also upload external datasets into your Google Earth Engine Assets with the NEW button. (See Earth Engine's Importing Raster Data for instructions on uploading an image to your assets or Importing Table/Shapefile Data for more details). We will export classification with the Export.image.toAsset() method. This method takes a dictionary with several parameters such as: image: the image object you want to export; description: a description to be showing on the Task tab. No spaces allowed; assetId: A name for your asset. No spaces allowed; scale: A scale to export; region: the region or area of your image you want to export. It takes a geometry/ feature collection; maxPixels: This is somewhat important argument. In order to avoid memory errors, the Earth Engine detault is 1x10e8 pixels. If you do not define your max pixels and your exports exceeds the default, you will get an error. We will create a geometry 'region' to encompass the entire classified scene in order to export the entire area. The code below will export the classification: Export.image.toAsset({ image: classification, description: &#39;ClassificationOutput&#39;, assetId: &#39;Cairo2022&#39;, scale: 30, region: region, maxPixels:1e12 }); Running the code above, will create the exporting task named ClassificationOutput in the Tasks tab. Figure 1.57: Tasks manager. Clicking the Run button will start exporting your classification. You can check some of the parameters of the Export.image.toAsset() one more time before exporting the asset. You are allowed to change some of these parameters at this point. For our example, everything will follow the parameters we defined previously. Figure 1.58: Tasks manager: Image Export. The time elapsed of your exporting task will be shown in the Tasks tab. Figure 1.59: Once a task has started the time elapsed will be shown in the Tasks tab. I may take several minutes to export your classification image to your assets. Once your task is completed, it will be shown at the Task tab as: Figure 1.60: Exported asset. Finally, your export will be available as an Earth Engine asset: Figure 1.61: Assets will be available through the Assets tab. If they are not showing, try refreshing the folder with the refresh button. 1.3.2 Example 2: 2015 - 2022 Map-to-Map change of greater Cairo and Giza area, Egypt In this example, we will use the codes from Example 1 to create a land cover classification for the year 2015 period. Following are the changes to be made on the codes from Example 1: The year object will have a value of '2015'; Repeat the training sample selection to make sure the samples reflect the correct land cover class for the L8 composite in the year 2015. You can access the edited script for two steps above here. After following the 2 steps above, you should achieve a similar output as before: Figure 1.62: Random Forest classification output for 2015. Export this new classification to you assets. Next, open a new code editor page and add the classification exports to this script by clicking the Import into script button (blue arrow) for both classification assets in the Assets tab: Figure 1.63: Assets tab with classification maps. Clicking in the Import into script button will add the assets to the script. Finally, rename each import by clicking on their names at the Imports header: Figure 1.64: Imports header. You can rename imported assets by clicking on its name. Alternatively, you can import assets to your scripts by using ee.Image() and their respective asset directory. For this example, you can use: var Cairo2015 = ee.Image(&quot;users/capacityBuilding/Cairo2015&quot;); var Cairo2022 = ee.Image(&quot;users/capacityBuilding/Cairo2022&quot;); Add Cairo2015 and Cairo2022 to the map using the code below: var paletteMAP = [ &#39;#0040ff&#39;, // Water - pixel value = 0 &#39;#00ab0c&#39;, // Croplands / Cultivated Areas - pixel value = 1 &#39;#fbf2ad&#39;, // Sand and bare areas - pixel value = 2 &#39;#878587&#39;, // Built-up and Urban Areas - pixel value = 3 ]; Map.addLayer(Cairo2015, {min: 0, max: 3, palette:paletteMAP}, &#39;Cairo2015&#39;); Map.addLayer(Cairo2022, {min: 0, max: 3, palette:paletteMAP}, &#39;Cairo2022&#39;); You can examine both layers on the map and try to identify areas where classes have changed in the 7 years period. However, we can use a transition error matrix to quickly quantify these changes. This matrix can be used to assess how much each class has changed and to what they have changed based on sampling design of your choosing. To do that, we will start by stacking both maps Cairo2015 and Cairo20222 into a single object stackedClassifications and rename Cairo2015 and Cairo2022 as 'before' and 'later', respectively: var stackedClassifications = Cairo2018.rename(&#39;later&#39;).addBands(Cairo2000.rename(&#39;before&#39;)); Next, we will create a testing sample set with 1000 points for each class using a stratified random sampling design. In this sampling design each class has the same weight. Therefore, the same number of points will be placed in each class, regardless of its relative extent. This is important to consider as some classes may have very small area extent to be captured by a random sampling design. In this example, water bodies is relatively small compared to the other three land cover classes. Thus, a stratified sampling design will ensure that this class will have the same number of points. We can used .stratifiedSample() method. This method takes a dictionary with several parameters such as: numPoints: Number of points you want to select by each class; classBand: The classification you want the sample points to inherit the class labels from; scale: Always remember to set the scale to match Landsat nomimal spatial resolution (30 m) Always remember to check the Docs tab for more information on the parameters each method takes! Using the information above, a stratified sample set can be created as following: var samples = stackedClassifications.stratifiedSample({ numPoints: 1000, classBand: &quot;before&quot;, scale: 30, geometries: true }); Once the points are placed, they will inherit the class label (pixel value) from Cairo2015 ('before'): We can add the samples to the map using Map.addLayer(). To colorize geometries, you use the color paramenter instead of palette: Map.addLayer(samples, {color:&#39;black&#39;}, &#39;Stratified samples&#39;); Figure 1.65: Stratified samples. 1000 samples were randomly selected within each strata (class). Next, we will create a transition matrix using the samples above. This matrix will show how many of these 1000 remained the same land cover class 7 years later (Cairo2022) and how many of them changed to a given class. You can use .accuracy() to calculate the % of these samples remained unchanged between the two periods: var transitionMatrix = samples.errorMatrix({ actual: &#39;before&#39;, predicted: &#39;later&#39; }); print(&#39;Transition Matrix&#39;, transitionMatrix); print(&#39;% of Unchanged Samples: &#39;, transitionMatrix.accuracy().multiply(100)); The matrix will be printed to the Console tab. The code above should produce something similar to figure below: Figure 1.66: Transition matrix based on the 4000 (1000 for each class) samples. In the transition matrix above you can draw information about how many samples transitioned to each class from 2015 to 2022. For example, from the 1000 samples of agriculture class (pixel value 1), 4 was classified (or transitioned) to water, 8 to sand and 53 to urban areas in 2022. 935 of them remained unchanged. Overall, 87% of these samples from all classes remained unchanged. Cropland area expansion and conversion in the Nile Delta from 2015-2022 Another quick analysis that can be done with land cover class maps in two points in time is highlighting areas in which a particular class has changed. In this example, we will assess the cultivated/cropland change between the 2015 and 2022. First, we will isolate the agricultural land class (pixel value 1) from both years into separate objects class2015 and class2022 using the .select() and .eq() methods. Secondly, we will calculate the change by simply subtracting both objects: var class2015 = Cairo2015.select([&#39;classification&#39;]).eq(1); var class2022 = Cairo2022.select([&#39;classification&#39;]).eq(1); var change = class2022.subtract(class2015); By default, the new objects class2015 and class2022 will have values of 1 where the 'classification' has the label equals (.eq) to 1 (agriculture) in the land cover map. This will be the case regardless of the label value. By using the .select() and .eq(), the new image object will default to value of 1 where that condition was met. Thus, the object change will have values of -1, 0 and 1, that represents loss/conversion, no change and gains, respectively: -1 = No crops in 2022 - crops in 2015 (0 - 1 = -1); 0 = crops in 2022 - crops in 2015 (1 - 1 = 0); 1 = crops in 2022 - no crops in 2015 (1 - 0 = 1) We will add change to the map. var paletteCHANGE = [ &#39;red&#39;, // Loss/conversion &#39;white&#39;, // No Change &#39;green&#39;, // Gain/Expansion ]; Map.addLayer(change, {palette: paletteCHANGE}, &#39;Change 2015-2022&#39;); The resulting change map will look similar to figure below. Green pixels represent 'gains' while red pixels represent 'conversions': Figure 1.67: Subset of the 2015-2022 Change Map. Conversion/loss of classifiedagricultural areas. This code can easily be used to investigate the other classes as well. Simply change the class value within .select(['classification']).eq(classValueHere). In the example below, we changed the value from 1 to 3 to highlight the expansion of urban/built up areas. Figure 1.68: Subset of the 2015-2022 Change Map. Expansion of the future capital of Egypt, New Cairo City. You can calculate the area of expansion/conversion by isolating the pixels of gain/loss from the change object into gain and loss. Then, calculate the area of each pixel using ee.Image.pixelArea() and multiplying by the count of pixels in gain and loss using multiply. The default unit is square meters (m²). You can use .divide() to transform into square kilometers (.divide(1000000)) or hectares (.divide(10000)): var gainArea = gain.multiply(ee.Image.pixelArea().divide(1000000)); var lossArea = loss.multiply(ee.Image.pixelArea().divide(1000000)); These objects will hold the area calculation for a single pixel. Then, we can use .reduceRegion() method, from the ee.Image() container. This will apply a reducer to all the pixels in a specific region. This method will take the reducer parameter, which will contain the type of mathematical operation you wish to compute for all these pixels. In this case, we will use ee.Reducer.sum(), to sum up all the single area values for all the pixels within gainArea and lossArea: Make sure to create a geometry around an area of interest in order to calculate the area (extent) for the pixels of a given class within that geometry! In this example, we used a geometry around New Cairo City called 'AOI'. var statsgain = gainArea.reduceRegion({ reducer: ee.Reducer.sum(), // Sum of all the area values. scale: 30, // Landsat scale. geometry: AOI, maxPixels: 1e14 }); var statsloss = lossArea.reduceRegion({ reducer: ee.Reducer.sum(), scale: 30, geometry: AOI, maxPixels: 1e14 }); Finally, you can print these values using the code below: print(statsgain.get(&#39;classification&#39;), &#39;km² of new built up areas in New Cairo City&#39;); Figure 1.69: Subset of the 2015-2022 Change Map. Expansion of the future capital of Egypt, New Cairo City. The .reduceRegion() method was used to calculate the extent of gains for the urban/built up class. Finally print() was used to print the result to the Console tab. 1.4 Post-classification processing So far you have learned the basics of creating a classification output using a machine learning algorithm such as Random Forest. The objective of this section is to provide useful post-classification steps for corrections and general improvement of a random forest classification output. 1.4.1 Re-classification Classification outputs often times will need some degree of correction or adjustment. Some of these corrections and adjustments include, for example, the Correction for missclassification erros in specific areas and changing class labels (pixel values) or class order. In this section we will explore the function remap(). This function maps from input values to output values, represented by two parallel lists: one includes the original number of classes and their value; the other represents which class (or classes) is being remapped and what it is being remapped to. To ilustrate this concept, consider the following example: The previously produced land cover maps include four classes: Water (pixel value = 0), Cropland / Cultivated Areas (pixel value = 1), Sand and bare areas (pixel value = 2) and Built-up and Urban Areas (pixel value = 3) (List 1). If any of these class values needs to be changed, the new value for that class is placed in the List 2, in the position of the class that needs changing (Figure 1). Figure 1.70: The remap function for the land cover classification of the Greater Cairo. To test this function, start by opening a new code editor page and importing one of the classification assets you used in the previous section. In this example, we will importe the latest land cover map from the year 2022 as a variable called 'Cairo2022' and we will add it to the map editor using the same color scheme from paletteMAP used in the previous section: var Cairo2022 = ee.Image(&quot;users/capacityBuilding/Cairo2022&quot;); var paletteMAP = [ &#39;#0040ff&#39;, // Water &#39;#00ab0c&#39;, // Croplands / Cultivated Areas &#39;#fbf2ad&#39;, // Sand and bare areas &#39;#878587&#39;, // Built-up and Urban Areas ]; Map.centerObject(Cairo2022); Map.addLayer(Cairo2022, {min: 0, max: 3, palette:paletteMAP}, &#39;Cairo2022&#39;); Using Map.centerObject() will center the map view on a given object you click the Run button. It is always a good practice to set the center on your main object so you can always come back to it! In this example you can center the map view to your classification object Cairo2022. Now, to illustrate the .remap() method, let's consider the following scenarios: *Scenario 1 A new version of the Cairo2022 map where Sand and bare areas (pixel value = 2) and Built-up and Urban Areas (pixel value = 3) switch orders in the final map. In this case, we can use the .remap() method to change the pixel value of Built-up and Urban Areas to 2 and Sand and bare areas to 3 in their respective position on list 2: Figure 1.71: Sand (2) and Cities (3) changing orders in the image output using .remap As a guide, the figure above can be used to create a new variable CairoV1 with the new order for the classes using .remap(): var CairoV1 = Cairo2022.remap([0,1,2,3],[0,1,3,2]); Add the CairoV1 to the the map using the original color scheme paletteMAP: Map.addLayer(CairoV1, {min: 0, max: 3, palette:paletteMAP}, &#39;Cairo V1&#39;); Figure 1.72: Cairo2022 and CairoV1 maps using the original paletteMap. Note that Sand and bare areas and Built-up and Urban areas switched colors as we switched their order. *Scenario 2 A new version of the Cairo2022 map where Sand and bare areas (pixel value = 2) and Built-up and Urban Areas (pixel value = 3) are merged into a new class called Barren land and Articicial Surfaces (pixel value = 2). Note that this new class can assume any value when merging, as long as both classes have the same value: Figure 1.73: Sand and bare areas merging with Built-up and Urban area into a new class value (2). New classes can assume any value when merging, as long as these values are the same for both classes. For this example, we will keep the value 2 for this new class in a new variable CairoV2 for the new map: var CairoV2 = Cairo2022.remap([0,1,2,3],[0,1,2,2]); We will add the CairoV2 to the the map using the original color scheme paletteMAP. Map.addLayer(CairoV2, {min: 0, max: 3, palette:paletteMAP}, &#39;Cairo V2&#39;); Note that the min and max range of values changed from 0-3 (4 classes) to 0-2 (three classes) in CairoV2. You can still maintain the original min and max range of values in this particular case because Map.addLayer() would only map the color scheme to the first three values ofCairov2 (0, 1 and 2). However, it is always good practice to set the min and max range of values to match the actual number of classes along with editing the color palette to have the same number of colors as the number of classes in your map. Figure 1.74: Cairo2022 and CairoV2 maps using the original paletteMap. Note that Sand and bare areas and Built-up and Urban areas have the same color as they were merged under the same pixel value (2). *Scenario 2 A new version of the Cairo2022 map where only a portion of it is remapped to a given class. This scenario is one of the most commonly used post-classification procedure where the goal is to remap specific areas to fix for classification errors. To ilustrate, we will consider the following example: Upon close inspection, the Cairo2022 map showed some cropland/cultivated areas within zones of dense urbanization of New Cairo City. A team on the ground went to the area and confirmed that is indeed dense urbanization and no agricultural land was found in that area. Therefore, that portion of the map needs to be rectified in order to reflect the actual land cover. In this case, the .where() method of an ee.Image() object is used. This function performs conditional replacement of values, following the formula input.where(test,value). For each pixel in each band of input, if the corresponding pixel in test is nonzero, Google Earth Engine will output the corresponding pixel in value, otherwise it will output the input pixel. Translating this function to this example, input is the land cover map in which we aim to perform the reclassification - in this case Cairo2020; test is the area or region of the map value will take place, and; value is the correct classification that will be included in the final map: Figure 1.75: The .where function applied to this example. In your own version of Cairo2022 map, create an new geometry over an area of your interest and name it aoi: Figure 1.76: A geometry over an area of Cairo2020 highlighting cultivated areas next to urbanization. The method .where() only uses ee.Image() objects as test. Therefore, using the geometry/feature aoi is not allowed. An easy and effective way to go around this rule is to create an image with ee.Image() and clip it for the region of interest: var region = ee.Image(1).clip(aoi); The function above will create an image with value of 1 and will clip it for the area of interest aoi. Next, using the same approach from Scenario 1 and 2, we will create a new version of Cairo2022 called ''subistitute', where the Cropland/Cultivated Areas class is remapped to Built-up and Urban Areas: the position 2 on list 2 (belonging to agricultural/cultivated areas) receives the value 3 from Built-up and Urban Areas on list 1: var substitute = Cairo2020.remap([0,1,2,3],[0,3,2,3]); Now, using .where() we will create a new map CairoV3 following the formula (1.75) and add it to the map editor: var CairoV3 = Cairo2022.where(region,substitute); Map.addLayer(CairoV3, {min: 0, max: 3, palette:paletteMAP}, &#39;Cairo V3&#39;); Compare the two classification objects: you will notice that every pixel of Cropland/Cultivated Areas (green, pixel value = 2) within the aoi in Cairo2022 is now remapped to Built-up and Urban Areas (grey, pixel value = 3) in CairoV3: Figure 1.77: CairoV3 showing the pixels of Cropland/Cultivated Areas remapped as Built-up and Urban Areas within aoi. 1.4.2 Map spatial smoothing Spatial smoothing is another post-classification procedure to enhance the quality of a land cover classification output. A common issue that arises from the pixel-based classification of fine/medium spatial resolution imagery is the salt-and-pepper effect. This happens when individual pixels are classified differently from their neighbors, creating speckles of isolated pixels of different classes. There are several ways to minimize this issue, such as: image pre-processing, such as low-pass filter and texture analysis; contextual classification, and; post-classification processing, such as median and mode filtering. In this section, we will focus on a post-classification technique to reduce the salt and pepper effect and edge roughness of land cover maps using focal median filtering (ee.Image.focal_median()). We will apply the .focal_median() method to Cairo2022. This method is a simple sliding-window spatial filter that replaces the center value (or class) in the window with the median of all the pixel values in the window. The window, or kernel, is usually square but can be any shape. The .focal_median() method usually is expressed as: Figure 1.78: .focal_median spatial filter. In the function above, the radius parameter specifies the number of pixels from the center that the kernel will cover. This radius value can be expressed as number of pixels or meters. The kernelType specifies the type of kernel to use. Either .focal_median(1, 'square', 'pixels') or .focal_median(30, 'square', 'meters') will produce a 3x3 pixel sliding-window, as you are specifing 1 pixel (or 30 meters which is equal to one Landsat pixel) in each direction from the center pixel. Similarly, a radius of 2 pixels or 60 meters will produce a 5x5 sliding-window: a center pixel plus 2 pixels in each direction. Following Figure 1.78 and the example above, we will apply the .focal_median() filtering with two radius sizes (30 and 60 meters) to Cairo2022, add Cairo30 and Cairo60 to the map editor and compare the results: var Cairo30 = Cairo2022.focal_median(30,&#39;square&#39;, &#39;meters&#39;); var Cairo60 = Cairo2022.focal_median(60,&#39;square&#39;, &#39;meters&#39;); Map.addLayer(Cairo30, {min: 0, max: 3, palette:paletteMAP}, &#39;Cairo 3x3&#39;); Map.addLayer(Cairo60, {min: 0, max: 3, palette:paletteMAP}, &#39;Cairo 5x5&#39;); Figure 1.79: Cairo2022 processed with .focal_median spatial filter two radius sizes. IMPORTANT: The .focal_median(1, 'square', 'pixels')* and .focal_median(30, 'square', 'meters') will only achieve the intended final result if you reproject it back to the original scale and projection from the original map. Note that this technique while effective in removing the &quot;salt-and-pepper&quot; effect from the original map, it creates edges that are not in the same resolution and projection as the original map. Therefore, creating fairly rounded boundaries for the classes. This is not ideal as it is usually intended for these maps to keep the same resolution as the original map, as well as to preserve the boundaries between classes. To account for this issue, you can reproject this output back the original scale so it is formed by 30 x 30 meter pixels. To do that, simply use .reproject(projection.atScale(scale)) within the .focal_median filter. The .reproject function will take two arguments: projection and scale. You can extract this information from the original map with .projection() and .nominalScale(): var projection = Cairo2022.projection(); var scale = projection.nominalScale(); Then, we re-aply the .focal_median() filter with .reproject(projection.atScale(scale)): var newCairo30 = Cairo2022.focal_median(1,&#39;square&#39;, &#39;pixels&#39;) .reproject(projection.atScale(scale)); var newCairo60 = Cairo2022.focal_median(60,&#39;square&#39;, &#39;meters&#39;) .reproject(prj.atScale(scale)); Figure 1.80: Cairo2022 processed with .focal_median spatial filter with 3x3 and 5x5 pixel windows. Note that the function was reprojected to match Cairo2022's projection and scale. Now the discrete boundaries of classes are pixelated back to 30 meters. Note that even though the boundaries of the classes are smoother, they are formed by the 30 x 30 meter pixels. Usually, a radius of 1 (3x3 square window) removes most of the salt-and-pepper effect, smoothens the boundaries in between classes while preserving the overall shape of the classes. Access the full scrip for this section here. "],["part-3-country-specific-applications.html", "Chapter 2 PART 3 - Country-specific Applications 2.1 Guinea 2.2 Liberia", " Chapter 2 PART 3 - Country-specific Applications In this part we will explore examples of class-specific classification using previously used functions as well as useful land cover classification-based analysis. 2.1 Guinea Description of Guinea here 2.1.1 Mangrove mapping in Guinea, West Africa In this example, we will use the codes from the previous parts for mangrove mapping in Guinea, in West Africa. We will also introduce the use of other datasets for masking and sample selection to assist in the classification workflow. We will start by opening a new code editor page and by defining the spatial and temporal parameters of the composite you wish to classify, just like the other examples. For the temporal parameters, lets use an annual 2021 composite: var year = 2021; var startDay = (year)+&#39;-01-01&#39;; var endDay = (year)+&#39;-12-30&#39;; For the spatial parameters, we can use the Large Scale International Boundary (LSIB) dataset and select Guineas national border. First, we will load the dataset into an object called 'countries' using the ee.FeatureCollection() object and the feature collection ID 'USDOS/LSIB/2013'. Secondly, we will filter the dataset using .filterMetadata() and select Guinea from the list of countries and storing it in aoi: Note that you can select any country border by filtering 'USDOS/LSIB/2013' dataset by using .filterMetadata('name' , 'equals', 'NAME OF THE COUNTRY IN CAPITAL LETTERS'). Alternatively, you can use any previously uploaded Guinea's boundary datasets available here. Select a dataset from the list, copy its Asset ID and load it with ee.FeatureCollection into aoi. Figure 2.1: Examples of Guinea's administrative borders. In this example, we aim to map of Guinea's mangroves. Therefore, we will choose prefectures5k as it encompasses the entirety of Guinea's coast: var aoi = ee.FeatureCollection(&#39;users/capacityBuilding/Guinea/prefectures5k&#39;); Important: All these datasets were made available through our collaboration with our colleagues in Guinea and the World Bank and they were modified with a GIS software for the sole purpose of this exercise. You can upload any geographical dataset (both raster or shapefile) into GEE with the NEW button in the Assets table. As auxiliary functions for cloud masking and spectral index calculation, we will use the same functions provided in PART 2: function maskClouds(image) { var cloudShadowBitMask = ee.Number(2).pow(3).int(); var cloudsBitMask = ee.Number(2).pow(4).int(); var QA = image.select(&#39;QA_PIXEL&#39;); var mask = QA.bitwiseAnd(cloudShadowBitMask).eq(0) .and(QA.bitwiseAnd(cloudsBitMask).eq(0)); return image.updateMask(mask).divide(100000).select(&quot;SR_B[0-9]*&quot;).copyProperties(image, [&quot;system:time_start&quot;]); } function addIndices(image) { var ndvi = image.normalizedDifference([&#39;SR_B5&#39;,&#39;SR_B4&#39;]).rename(&#39;NDVI&#39;); var nbr = image.normalizedDifference([&#39;SR_B5&#39;,&#39;SR_B7&#39;]).rename(&#39;NBR&#39;); var ndmi = image.normalizedDifference([&#39;SR_B7&#39;,&#39;SR_B3&#39;]).rename(&#39;NDMI&#39;); var mndwi = image.normalizedDifference([&#39;SR_B3&#39;,&#39;SR_B6&#39;]).rename(&#39;MNDWI&#39;); var sr = image.select(&#39;SR_B5&#39;).divide(image.select(&#39;SR_B4&#39;)).rename(&#39;SR&#39;); var bare = image.normalizedDifference([&#39;SR_B6&#39;,&#39;SR_B7&#39;]).rename(&#39;BI&#39;); var gcvi = image.expression(&#39;(NIR/GREEN)-1&#39;,{ &#39;NIR&#39;:image.select(&#39;SR_B5&#39;), &#39;GREEN&#39;:image.select(&#39;SR_B3&#39;) }).rename(&#39;GCVI&#39;); var evi = image.expression( &#39;2.5 * ((NIR-RED) / (NIR + 6 * RED - 7.5* SR_BLUE +1))&#39;, { &#39;NIR&#39;:image.select(&#39;SR_B5&#39;), &#39;RED&#39;:image.select(&#39;SR_B4&#39;), &#39;SR_BLUE&#39;:image.select(&#39;SR_B2&#39;) }).rename(&#39;EVI&#39;); var msavi = image.expression( &#39;(2 * NIR + 1 - sqrt(pow((2 * NIR + 1), 2) - 8 * (NIR - RED)) ) / 2&#39;, { &#39;NIR&#39;: image.select(&#39;SR_B5&#39;), &#39;RED&#39;: image.select(&#39;SR_B4&#39;)} ).rename(&#39;MSAVI&#39;); return image .addBands(ndvi) .addBands(nbr) .addBands(ndmi) .addBands(mndwi) .addBands(sr) .addBands(evi) .addBands(msavi) .addBands(gcvi) .addBands(bare); } 1) Masking In this example, we will focus on mangrove forest mapping. Therefore, other known classes can be masked from the analysis. var globalwater = ee.Image(&#39;JRC/GSW1_0/GlobalSurfaceWater&#39;); The Global Water Dataset 'JRC/GSW1_0/GlobalSurfaceWater' has different bands: one of them 'occurrence'. This band shows how many times (expressed as %) a given pixel was classified as water relative to the total time span of the dataset. Lets isolate the 'occurrence' band from the globalwater object: var occurrence = globalwater.select(&#39;occurrence&#39;); Masks are composed by zeros and non-zero values. When you set or apply a mask to an image, the output image will keep its original values where the mask has non-zero values and pixels will be masked where the mask has zeros: Figure 2.2: Masking procedure. In this figure, the mask was applied to the raster image generating an output image where pixels are visible only where the correspondent mask pixel has non-zero values. For this example, we want to create a water mask. Thus, the water mask has to have zeros where there is water and non-zero values for non-water pixels. Consequently, when we apply this mask to a Landsat image, pixels of water will be invisible (transparent) while all the other pixels will remain visible in the composite. For the mask using the JRC Global Water dataset, make sure you are selecting &quot;permanent&quot; water. One way of doing this is by filtering the dataset for water pixels that occurred more than 50% of the time over the 35 years time spam for this dataset. You can be more or less restrictive with the water extent by changing the 50% threshold. var waterMask = occurrence.lt(50).unmask(1); Note that .lt(50) was used to select pixels from occurrence that are smaller (or lower) than 50%. Automatically, the values above 50% will be set to 0 which is what is needed for this mask. In this particular case, we use .unmask(1) to set to 1 (or unmask) all the other areas that were originally masked in the JRC Global Water dataset. Figure 2.3: Global water mask produced with JRCs Global Surface Water dataset and Guinea's coastal prefectures (aoi) in red for reference. You can add this mask to the map editor (and clip for aoi) using Map.addLayer(waterMask.clip(aoi), {}, 'Water Mask'). Areas in black (0) will be masked in the composite while areas in white (1) will remain. Elevation mask The purpose of this mask is to further remove pixels that are unlikely to be mangrove forests based on altitude values. Generally, mangroves will occur near shore where elevation and slope are relatively low. Similar to the water mask, we will create a mask using the SRTM Elevation Data: var srtm = ee.Image(&#39;USGS/SRTMGL1_003&#39;); Similarly to the previous dataset, we will select the band of interest by using the .select() method. The altitude values for the SRTM dataset are stored in the elevation band called 'elevation'. We will create a mask 'elevMask' where pixels that have elevation values greater than 25 meters are removed. For that, you select everything that is lower than (.lte) 25 meters; any other value above 25 meters will be set to 0 automatically when using .select(): var elevation = srtm.select(&#39;elevation&#39;); var elevMask = elevation.lte(25); Figure 2.4: Global elevation (&gt; 25 m) mask produced with the Shuttle Radar Topography Mission dataset and Guinea's coastal prefectures (aoi) in red for reference. You can add this mask to the map editor (and clip for aoi) using Map.addLayer(elevMask.clip(aoi), {}, 'Elevation Mask'). Areas in black (0) will be masked in the composite while areas in white (1) will remain. Code Checkpoint 2) Landsat 8 Image Collection and Cloud-free Mosaic Similarly to the examples from PART 2, we will load the Landsat 8 Surface Reflectance data archive into an object called 'collection' by using the container ee.ImageCollection() and the collection ID 'LANDSAT/LC08/C02/T1_L2'. Secondly, we will filter this image collection for the temporal parameters by using the method .filterDate(). Finally, we will map the cloud-masking and spectral indices functions to the collection: var collection = ee.ImageCollection(&#39;LANDSAT/LC08/C02/T1_L2&#39;) .filterDate(startDay,endDay) .map(maskClouds) .map(addIndices); Then, in an object called 'composite', we will reduce the image collection into an annual composite using the .median() method. Next, we will mask the composite using the masks waterMask and elevMask using .mask() method for the first mask and then .updateMask() for the second. This container is necessary as we are updating the raster that will have been masked by the first mask. Figure 2.5: Output raster from .mask() and .updateMask() of a previously masked input raster. .updateMask() will only mask areas that have not been masked previously; if .mask() is used for a second mask, areas that have been previously invisible (i.e masked) will then assume the values of the mask used. As a rule of thumb: the first mask applied to the raster is done by .mask() and then use .updateMask() for any subsequent masks. Finally, we will clip the composite to our area of interest aoi using the container .clip() method: var composite = collection .median() .mask(waterMask) .updateMask(elevMask) .clip(aoi); Setting your code with indents is very helpful!. It will allow you to &quot;turn on&quot; and &quot;turn off&quot; certain portions of a code with comment bars. In the example above, you can turn off any mask by just adding // before the full period. var composite = collection .median() //.mask(waterMask) //.updateMask(elevMask) .clip(aoi); The code above is just for illustration purposes! Remember to remove the // from the portion of the code that you want Earth Engine to read. Finally, add the composite to the Map editor using the code below. Map.centerObject(aoi); Map.addLayer(composite, {bands: [&#39;SR_B5&#39;, &#39;SR_B6&#39;, &#39;SR_B4&#39;], min:0.1, max:0.2}, &#39;Composite&#39;); This band composition (RGB 564) is a good composition to highlight mangrove forests: Figure 2.6: Masked composite displayed with an RGB564 composition highlighting mangrove forests in dark red. Notice that areas of open water and higher altitudes were masked as they are not relevant to our example. Unmasked composite was added for comparison purposes: you can take advantage of indenting to turn masks on/off with // comment bars. Code Checkpoint 3) Supervised Classification with Random Forest Strata for Sample Selection As shown in Part 1 and Part 2, the first step to perform a supervised classification is to select the training samples. One way of selecting the training samples for the Random Forest classifier was shown in the training sample selection section of Part 2. Alternatively, you can automatically select random points based on a stratification map using .stratifiedSample(). In this case, the stratification map will have two classes: Mangroves and Non-mangroves. You can create a mangrove stratum for sample selection using available mangrove datasets, such as: Global Mangrove Forest Distribution dataset (raster) for the year 2000, available through Google Earth Engine (ee.ImageCollection('LANDSAT/MANGROVE_FORESTS')), or; The latest global Mangrove Extent from the Global Mangrove Watch, available for download here. Or any other dataset that you may have available; Mangroves: For this example, we will use the GMW Global mangrove extent dataset for 2016 that was previously downloaded and added as an asset to Google Earth Engine. First, load the mangrove extent shapefile into a ee.FeatureCollection object: var mangroveDataset = ee.FeatureCollection(&#39;users/capacityBuilding/Guinea/Mangroves2016&#39;); As seen in Part 2, to create the mangrove stratification class, we simply create an image with ee.Image(1) and clip it using mangroveDataset: var mangrove = ee.Image(1).clip(mangroveDataset); Non-mangroves: Using .where() (See its formula here: 1.75), we will create an image of zeros where the value two (2) will be added where there is a pixel of any band from the composite: var nonmangrove = ee.Image(0).where(composite.select(&#39;SR_B1&#39;),2).selfMask(); Notice that a new method was used in the code above. The method .selfMask() is an method from an ee.Image() object that is used when you want to mask an image with itself. In other words, when an ee.Image has zeros in it, you can use .selfMask() to remove the zeros, just like an other mask. Finally, to create a stratification classes map called 'strata', we will use .where() to set nonmangrove pixels to 1 where it overlaps with pixels of the mangrove stratum (mangrove) and use .rename() to rename it to 'stratificationClass': var strata = nonmangrove.where(mangrove,1).rename(&#39;stratificationClass&#39;); The object strata is an image with one band called 'stratificationClass', with Mangrove pixes have value of 1 and non-mangrove pixels have value of 2. It is good practice to rename this band to something easy to remember because .stratifiedSample() requires the image and the band name with which the samples will be selected. Add strata to the map editor to visualize: Map.addLayer (strata, {palette:[&#39;#6D8B74&#39;,&#39;#D0C9C0&#39;], min:1, max:2}, &#39;Stratification Map&#39;); Figure 2.7: Stratification map strata showing Global Mangrove Watch's 2016 Mangrove extent (stratificationClass: 1) and non-mangrove areas (stratificationClass: 2). The .stratifiedSample() method will generate a set of random points to each of these classes. Random training sample selection Now that we have the stratification map strata for mangroves and all other areas, we will use .stratifiedSample() method below to select 1000 points (classPoints) for each of the two classes (classValues): var stratified = strata.addBands(ee.Image.pixelLonLat()).stratifiedSample({ numPoints: 1, classBand: &#39;stratificationClass&#39;, scale: 30, region: aoi, classValues:[1,2], classPoints:[1000,1000] }).map(function(f) { return f.setGeometry(ee.Geometry.Point([f.get(&#39;longitude&#39;), f.get(&#39;latitude&#39;)])); }); The code above includes: .addBands(ee.Image.pixelLonLat()) to add a band to strata that will have the latitude and longitude for each pixel; numPoints is the default number of points to sample in each class in strata. If numPoints is set to 2000, 1000 points will be selected for each of the two classes in strata. This, however, can be overridden for specific classes using the classValues and classPoints properties. In this case, these properties allow which classes you want to use from your stratification map and how many points for each of these classes you want to select. classBand: 'stratificationClass' to define the band from strata that will be used to select the random samples; scale sets the scale to 30 meters to match Landsat nominal spatial resolution; The rest of the code will set these samples as geometry and get their lat/long coordinates. You can use the code below to colorize these point samples based on a color palette for visualization. First, the color pallete for this particular case will be slightly different that the color palettes that have been used throught this tutorial so far: it will be created as a ee.List() of colors including a null color: var paletteSamples = ee.List([ &#39;FFFFFF&#39;, // NULL &#39;6D8B74&#39;, // Mangrove &#39;D0C9C0&#39;, // Non-Mangrove ]); Then, a 'features' object will be created to include the colorized version of stratified based on the code: var features = stratified.map(function(f) { var landcover = f.get(&#39;stratificationClass&#39;); return ee.Feature(ee.Geometry.Point([f.get(&#39;longitude&#39;), f.get(&#39;latitude&#39;)]), f.toDictionary()).set({style: {color: paletteSamples.get(landcover) }}); }); Finally, we will add features to the map editor using Map.addLayer and .style() following the formula below: Map.addLayer(features.style({styleProperty: &quot;style&quot;}),{}, &#39;Samples/Location&#39;); Remember, these steps are not necessary to the classification workflow. However, it allows you to visualize the selected samples with a color palette of your choice. Figure 2.8: Stratified random samples. The .stratifiedSample() method generates a set of random points to each of these strata. Code Checkpoint Classification As seen in in Part 2, one of the first steps to train a Random Forest classifier is select the predictors to assign to each sample point in the sample set. For this example, we will create a list ('bands') with the names of three spectral bands ('SR_B4','SR_B5','SR_B6') and the spectral indices ('NDVI','NBR','MNDWI','SR','GCVI' and 'MSAVI'). var bands = [&#39;SR_B4&#39;,&#39;SR_B5&#39;,&#39;SR_B6&#39;,&#39;NDVI&#39;,&#39;NBR&#39;,&#39;MNDWI&#39;,&#39;SR&#39;,&#39;GCVI&#39;,&#39;MSAVI&#39;]; var samplesAutomatic = composite.select(bands).sampleRegions({ collection: stratified, properties: [&#39;stratificationClass&#39;], scale: 30, geometries: true, }); A second sample set ('groundtruth') will be used to illusrate a validation process for the mangrove extent map. First, we will load a pre-selected set of samples using ee.FeatureCollection() then create 'samplesgroundtruth' similarly to the previous set of samples: var groundtruth = ee.FeatureCollection(&#39;users/capacityBuilding/Guinea/Groundtruth&#39;); If you print groundtruth to the Console tab, you will see that its property is called landcover. Therefore, in the code below, the argument properties will be a 'landcover': var samplesgroundtruth = composite.select(bands).sampleRegions({ collection: groundtruth, properties: [&#39;landcover&#39;], scale: 30, geometries: true, }); Next, we will train a Random Forest classifier using the samplesAutomatic as training samples. In this example, we will use 200 trees and 8 predictors to be tested at each tree node: var RandomForest = ee.Classifier.smileRandomForest(200,8).train({ features: samplesAutomatic, classProperty: &#39;stratificationClass&#39;, inputProperties: bands }); Finally, we will classify composite into a mangrove extent map using the trained classifier and add the output to the map editor using a color palette of your choice: var classification = composite.select(bands).classify(RandomForest); var paletteMAP = [ &#39;01937C&#39;, // Mangrove &#39;B6C867&#39;, // Non-Mangrove ]; Map.addLayer (classification, {min: 1, max: 2, palette:paletteMAP}, &#39;Classification with Automatic Samples&#39;); Figure 2.9: Random forest classification output produced with automatic sample selection Code Checkpoint Validation For the validation of the mangrove extent map we will use .classify() and .errorMatrix().The .classify method will classify the ground truth samples with the trained random forest classifier; .errorMatrix() will compare the result of this classification with their own class label: var validation = samplesgroundtruth.classify(RandomForest); var testAccuracy = validation.errorMatrix(&#39;landcover&#39;, &#39;classification&#39;); Print the results to the Console: print(&#39;Map Overall Accuracy: &#39;, testAccuracy.accuracy()); print(&#39;Kappa: &#39;, testAccuracy.kappa()); print(&#39;Validation error matrix Map: &#39;, testAccuracy); Figure 2.10: Validation results for this example. Ground truth data (in this context it refers to independent samples) were used to calculate the overall accuracy and kappa values. 2.2 Liberia In this example, we will use the codes from the previous sections for (a) mapping and detecting individual classes such as plantations; (b) a class by class land cover mapping approach. 2.2.1 Anomaly-based plantation detection Spectral confusion between planted tree crops with native vegetation is a well-known challenge in using moderate resolution optical remote sensing data for mapping plantations. Cultivated land in Liberia primarily consists of woody commodity crops (oil palm and rubber trees). Even with the availability of forest concessions datasets to constrain the classification of these crops, the Random Forest relies on spectral predictors that are almost identical to native vegetation. Aditionally, small scale plantations intermixed with evergreen forest will also add to the challenges of separating these classes. However, areas of plantation usually undergo harvesting cycles, which creates a temporal signature that is different than evergreen natural vegetation: every few years, the spectral signature of these areas will show traces of bare soil due to harvesting and replanting. We can take advantage of this interannual differences by doing an anomaly analysis. By definition, anomaly is anything that deviates from what is standard, normal, or expected. Usually, anomalies are calculated by subtracting a long-term average of a variable from the actual value of that variable at a given time. For example, if X = actual value of average NDVI for vegetation in 2020, and Y = long-term average NDVI of vegetation (an average over many years), then the anomaly = X  Y. If the anomaly values are zero (or very close to zero), it means that NDVI remained relatively stable in that period, which indicates that there has not been any significant disturbance in that area. On the other hand, a positive anomaly means that the NDVI signal is greater than its long-term average, which indicates that vegetation has shown growth in that area; similarly, a negative anomaly means that the NDVI signal is weaker than its long-term average, indicating a potential loss in the area. This is a very useful tool to detect changes over time. Depending on where and how these changes happen over time, you can then conclude that that area is being planted and harvested. Next, we will provide several blocks of code to perform a quick anomaly analysis over Liberia and potentially identifying areas of plantation. Harmonizing Landsat 5/7/8 Image Collections The Landsat TM/ETM+ and OLI sensors present differences between their spectral characteristics. Thus, to ensure inter-sensor harmonized spectral information and temporal continuity, we will harmonize the entire Landsat image archive using the statistical functions presented in Roy et al. (2016). For that, we will start by defining the temporal parameters for the harmonization: var startYear = 1984; var endyear = 2021; var startDay = &#39;01-01&#39;; var endDay = &#39;12-31&#39;; Here we used the entire Landsat archive. Next, we will create several functions for the harmonization of the Landsat archive: Harmonization Function: harmonizationRoy uses the regression coefficients (slopes and intercepts) retrieved from Roy et al. (2016) and performs a linear transformation of ETM+ spectral space to OLI spectral space: var harmonizationRoy = function(oli) { var slopes = ee.Image.constant([0.9785, 0.9542, 0.9825, 1.0073, 1.0171, 0.9949]); var itcp = ee.Image.constant([-0.0095, -0.0016, -0.0022, -0.0021, -0.0030, 0.0029]); var y = oli.select([&#39;B2&#39;,&#39;B3&#39;,&#39;B4&#39;,&#39;B5&#39;,&#39;B6&#39;,&#39;B7&#39;],[&#39;B1&#39;, &#39;B2&#39;, &#39;B3&#39;, &#39;B4&#39;, &#39;B5&#39;, &#39;B7&#39;]) .resample(&#39;bicubic&#39;) .subtract(itcp.multiply(10000)).divide(slopes) .set(&#39;system:time_start&#39;, oli.get(&#39;system:time_start&#39;)); return y.toShort(); }; Retrieve a Particular Sensor Function: getSRcollection will be used to retrieve individual sensor collections based on the temporal parameters and the harmonization function above. Additionally, this function will mask cloud, cloud shadow, and snow based on the Landsat quality assessment bands: var getSRcollection = function(year, startDay, endYear, endDay, sensor) { var srCollection = ee.ImageCollection(&#39;LANDSAT/&#39;+ sensor + &#39;/C01/T1_SR&#39;) .filterDate(year+&#39;-&#39;+startDay, endYear+&#39;-&#39;+endDay); srCollection = srCollection.map(function(img) { var dat = ee.Image(ee.Algorithms.If( sensor == &#39;LC08&#39;, harmonizationRoy(img.unmask()), img.select([&#39;B1&#39;, &#39;B2&#39;, &#39;B3&#39;, &#39;B4&#39;, &#39;B5&#39;, &#39;B7&#39;]) .unmask() .resample(&#39;bicubic&#39;) .set(&#39;system:time_start&#39;, img.get(&#39;system:time_start&#39;)) ) ); // Cloud, cloud shadow and snow mask var qa = img.select(&#39;pixel_qa&#39;); var mask = qa.bitwiseAnd(8).eq(0).and( qa.bitwiseAnd(16).eq(0)).and( qa.bitwiseAnd(32).eq(0)); return dat.mask(mask); }); return srCollection; }; Combining the Collections Function: getCombinedSRcollection will merge all the individual L5/L7/L8 collections into one: var getCombinedSRcollection = function(year, startDay, endYear,endDay) { var lt5 = getSRcollection(year, startDay, endYear, endDay, &#39;LT05&#39;); var le7 = getSRcollection(year, startDay, endYear, endDay, &#39;LE07&#39;); var lc8 = getSRcollection(year, startDay, endYear, endDay, &#39;LC08&#39;); var mergedCollection = ee.ImageCollection(le7.merge(lc8).merge(lt5)); return mergedCollection; }; Vegetation Indices: addIndices calculates several vegetation/spectral indices based on the harmonized Landsat bands. In this example, we are including the Normalized Difference Vegetation Index (NDVI), the Enhanced Vegetation Index (EVI), the Soil Adjusted Vegetation Index (SAVI), the Normalized Difference Mangrove Index (NDMI), the Normalized Difference Water index (NDWI), the Modified Normalized Difference Water Index (MNDWI) and many others. Here is where you can include your own vegetation indices: var addIndices = function(image) { var ndvi = image.normalizedDifference([&#39;B4&#39;, &#39;B3&#39;]).rename(&#39;NDVI&#39;); var evi = image.expression( &#39;2.5 * ((NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))&#39;, { &#39;NIR&#39;: image.select(&#39;B4&#39;), &#39;RED&#39;: image.select(&#39;B3&#39;), &#39;BLUE&#39;: image.select(&#39;B1&#39;) }).rename(&#39;EVI&#39;); var savi = image.expression( &#39;((NIR - RED) / (NIR + RED + 0.5) * (0.5 + 1))&#39;, { &#39;NIR&#39;: image.select(&#39;B4&#39;), &#39;RED&#39;: image.select(&#39;B3&#39;), &#39;BLUE&#39;: image.select(&#39;B1&#39;) }).rename(&#39;SAVI&#39;); var ndmi = image.normalizedDifference([&#39;B7&#39;,&#39;B2&#39;]).rename(&#39;NDMI&#39;); var nbr = image.normalizedDifference([&#39;B4&#39;, &#39;B7&#39;]).rename(&#39;NBR&#39;); var ndwi = image.normalizedDifference([&#39;B5&#39;,&#39;B4&#39;]).rename(&#39;NDWI&#39;); var mndwi = image.normalizedDifference([&#39;B2&#39;,&#39;B5&#39;]).rename(&#39;MNDWI&#39;); return image.addBands(ndvi) .addBands(evi) .addBands(savi) .addBands(ndmi) .addBands(nbr) .addBands(ndwi) .addBands(mndwi); }; Finally, collectionL5L7L8 will include the final harmonized collection with all the sensors based on the temporal parameters defined previously, with all spectral bands and vegetation/spectral indices, filtered to include only scenes over Liberia: var aoi = ee.FeatureCollection(&#39;users/capacityBuilding/Liberia/LiberiaCounties&#39;); var collectionL5L7L8 = getCombinedSRcollection(startYear, startDay, endyear, endDay).map(addIndices).filterBounds(aoi); Code Checkpoint Vegetation Index Anomaly To calculate the anomaly, we will start by defining the index we want to compute the anomaly for and the reference period to get the average value of that index over time. In this example, we will use the Normalized Burn Ratio (NBR) index, which is useful to highligh burned / bare areas and var index = &#39;NBR&#39;; var ref_start = &#39;1985-01-01&#39;; var ref_end = &#39;2010-12-31&#39;; Next, we will create the reference collection using collectionL5L7L8 and the parameters above. You can print the size of this reference collection to the console using print() and .size(): var reference = collectionL5L7L8 .filterDate(ref_start, ref_end) .select(index) .sort(&#39;system:time_start&#39;, true); print(&#39;Number of images in Reference Collection&#39;, reference.size()); Figure 2.11: Number of scenes in the reference collection that will be used to calculate the reference value for the anomaly. We can now calculate the mean value (and other statistics) for the reference collection reference using the .median() and .mean() methods: var mean = reference.mean().clip(aoi); var median = reference.median().clip(aoi); Now that we have the long-term reference metrics, we can define the period for which you want to compute the the anomaly. In this example, we will use yearly intervals starting in 2016 until 2020. We are interested to see where we can observe cycles of planting and harvesting for plantations in Liberia. However, any combination of years is possible depending on what period you are interested in. Then, an anomaly function can be created to subtract the metric from the average of your period of interest: var period_start = &#39;2016-01-01&#39;; var period_end = &#39;2016-12-30&#39;; var anomalyFunction = function(image){ return image.subtract(median) .set(&#39;system:time_start&#39;, image.get(&#39;system:time_start&#39;))}; Finally, we will map the anomalyFunction to the Landsat collection collectionL5L7L8 filtered by the period_start and period_end: var series = collectionL5L7L8.filterDate(period_start, period_end).map(anomalyFunction); The object series will have all the spectral bands and vegetation/spectral indices for the time period defined above. Their values, however, will be different from the original collection since we subtracted the average value of the reference period. The next step is to sum all the values for the index from series and divide by the number of images available. You can achieve that by using the .sum() and .count() methods: var seriesSum = series.select(index).sum(); var numImages = series.select(index).count(); var anomaly = seriesSum.divide(numImages); So far, the steps we have taken were: Created a reference value (median) for the NBR index for the 1985-2010 period; Then subtracted this median value from every scene in the period of interest (year 2016) Averaged the anomaly values for that year by adding all values and dividing by the number of observations (images within that period). We can add the object anomaly to the Map Editor using a color ramp: var visAnom = { min: -0.20, max: 0.20, palette: [&#39;#481567FF&#39;, &#39;#482677FF&#39;, &#39;#453781FF&#39;, &#39;#404788FF&#39;, &#39;#39568CFF&#39;, &#39;#33638DFF&#39;, &#39;#2D708EFF&#39;, &#39;#287D8EFF&#39;,&#39;#238A8DFF&#39;, &#39;#1F968BFF&#39;, &#39;#20A387FF&#39;, &#39;#29AF7FFF&#39;, &#39;#3CBB75FF&#39;,&#39;#55C667FF&#39;, &#39;#73D055FF&#39;, &#39;#95D840FF&#39;, &#39;#B8DE29FF&#39;, &#39;#DCE319FF&#39;,&#39;#FDE725FF&#39; ] }; Map.addLayer(anomaly, visAnom, index + &#39; anomaly&#39;); Figure 2.12: Normalized Burn Ratio (NBR) index anomaly over Liberia. Areas of negative anomaly are shown in purple/dark blue. A negative anomaly (in purple in the figure above) means that the NBR signal in 2016 is weaker than its long-term average, indicating a potential loss in the area for that year. Notice that in some areas you can see regular shapes, which means that those losses are potential harvested plantation. On the other hand, positive anomaly (in yellow) will show areas where soil is no longer exposed, giving us an indication of potential areas being replanted. Not every negative anomaly will be harvesting of planted areas nor every positive anomaly will be areas being replanted. The anomaly can give us an indication of where these areas MAY be, based on the pattern observed. Investigate the following years by changing the values for period_start and period_end. By doing so, we can have an informed idea where these planted areas are, which would be rather challenging to detected using only the Landsat composite. We will export each anomaly individually. For that, change the values for period_start and period_end for the year 2016 and use the code below to export the anomaly: Export.image.toAsset({ image: anomaly, description: &#39;Anomaly2016&#39;, assetId: &#39;Anomaly2016&#39;, crs:&#39;EPSG:4326&#39;, crsTransform:[0.0002777777777777778,0,-180.0001388888889,0,-0.0002777777777777778,60.00013888888889], maxPixels:1e13, region: aoi, }); Repeat the process for 2017, 2018, 2019 and 2020. * Planted areas On a new Code Editor page, import the anomalies to the script. var nbr16 = ee.Image(&#39;users/celiohelder/Liberia/NBR_Anomaly2016&#39;); var nbr17 = ee.Image(&#39;users/celiohelder/Liberia/NBR_Anomaly2017&#39;); var nbr18 = ee.Image(&#39;users/celiohelder/Liberia/NBR_Anomaly2018&#39;); var nbr19 = ee.Image(&#39;users/celiohelder/Liberia/NBR_Anomaly2019&#39;); var nbr20 = ee.Image(&#39;users/celiohelder/Liberia/NBR_Anomaly2020&#39;); As mentioned previously, not every negative anomaly will be harvesting of planted areas nor every positive anomaly will be areas being replanted. The anomaly can give us an indication of where these areas may be, based on the pattern observed. We can also focus on concessions. A concession is a license, permit, or other contract that gives private companies the rights to establish a plantation on public land. Here we will also focus on areas of Rubber and Oil Palm Concessions. The data was previously uploaded into GEE and you can have access to it by loading it into your script with ee.FeatureCollection('users/capacityBuilding/Liberia/Rubber') and ee.FeatureCollection('users/capacityBuilding/Liberia/OilPalm'): var rubber = ee.FeatureCollection(&#39;users/capacityBuilding/Liberia/Rubber&#39;); var oilpalm = ee.FeatureCollection(&#39;users/capacityBuilding/Liberia/OilPalm&#39;); var concessions = rubber.merge(oilpalm); Add the anomalies and the concessions data to the Map Editor: var visAnom = { min: -0.20, max: 0.20, palette: [&#39;#481567FF&#39;, &#39;#482677FF&#39;, &#39;#453781FF&#39;, &#39;#404788FF&#39;, &#39;#39568CFF&#39;, &#39;#33638DFF&#39;, &#39;#2D708EFF&#39;, &#39;#287D8EFF&#39;,&#39;#238A8DFF&#39;, &#39;#1F968BFF&#39;, &#39;#20A387FF&#39;, &#39;#29AF7FFF&#39;, &#39;#3CBB75FF&#39;,&#39;#55C667FF&#39;, &#39;#73D055FF&#39;, &#39;#95D840FF&#39;, &#39;#B8DE29FF&#39;, &#39;#DCE319FF&#39;,&#39;#FDE725FF&#39; ] }; Map.addLayer(nbr16,visAnom, &#39;NBR anomaly 2016&#39;); Map.addLayer(nbr17,visAnom, &#39;NBR anomaly 2017&#39;); Map.addLayer(nbr18,visAnom, &#39;NBR anomaly 2018&#39;); Map.addLayer(nbr19,visAnom, &#39;NBR anomaly 2019&#39;); Map.addLayer(nbr20,visAnom, &#39;NBR anomaly 2020&#39;); Map.addLayer(ee.Image().paint(oilpalm, 0, 2), {palette:[&#39;black&#39;]}, &#39;OilPalm&#39;); Map.addLayer(ee.Image().paint(rubber, 0, 2), {palette:[&#39;purple&#39;]}, &#39;Rubber&#39;); Figure 2.13: 2016 NBR Anomaly and the oil palm and rubber plantation concessions. Note that even before the addition of the concession dataset, we could outline potential areas of plantation based on the anomaly values. Now, overlaying these concessions borders with the anomaly layer, we have strong evidence that those are indeed planted areas. Due to their distinct anomaly values compare do their surrounding classes and their distinct pattern (particularly in the area within the concessions), we are able to separate them with value threshold. Using the Inspector tab, investigate the anomaly values for the dark blue/purple (negative anomaly) and the yellow (positive anomaly) areas. Upon close inspection, we concluded lower values than -0.1 can capture the areas of losses; on the other hand, values greater than 0.1 capture the areas of gains. We can apply the conditionals .lte() and .gt() to the NBR Anomaly layers to extract these areas: var lossanomaly1617 = nbr16.lt(-0.10); var gainanomaly1617 = nbr16.gt(0.10); var lossanomaly1718 = nbr17.lt(-0.10); var gainanomaly1718 = nbr17.gt(0.10); var lossanomaly1819 = nbr18.lt(-0.10); var gainanomaly1819 = nbr18.gt(0.10); var lossanomaly1920 = nbr19.lt(-0.10); var gainanomaly1920 = nbr19.gt(0.10); var lossanomaly2021 = nbr20.lt(-0.10); var gainanomaly2021 = nbr20.gt(0.10); Map.addLayer(lossanomaly1617.selfMask(), {palette:&#39;F94C66&#39;}, &#39;LossNBRanomaly2016-2017&#39;); Map.addLayer(gainanomaly1617.selfMask(), {palette:&#39;F7EC09&#39;}, &#39;GainNBRanomaly2016-2017&#39;); Figure 2.14: Areas of losses/harvest and gains/planting based on threshold values of NBR anomaly. By stacking 2016, 2017, 2018, 2019 and 2020 losses and gains and using the concession datasets as a constraint, we can create a Plantation layer for Liberia. We can construct this layer in a way to include the years where harvest and planting was observed. To do so, we will add the layers together. However, we have to set a flag value to each year. The steps below presents a simplistic way to create this flag: First, we will multiply each loss and gains layer by 10000,1000,100,10 and 1: var loss16 = lossanomaly1617.multiply(10000); var loss17 = lossanomaly1718.multiply(1000); var loss18 = lossanomaly1819.multiply(100); var loss19 = lossanomaly1920.multiply(10); var loss20 = lossanomaly2021.multiply(1); Now, we can stack these layers using .add(). The result will be a layer with values raging from 1 to 11111, where the position of the digit represents the year and the value 0 and 1 represent absence and presence, respectively. For example, a pixel with value of 10110 in this flag will mean that losses/harvest derived from the negative anomaly were observed in 2016, not in 2017, in 2018 and in 2019 and not in 2020. var lossFlag = loss16.add(loss17) .add(loss18) .add(loss19) .add(loss20); Similarly, we can create a gains flag: var gain16 = gainanomaly1617.multiply(10000); var gain17 = gainanomaly1718.multiply(1000); var gain18 = gainanomaly1819.multiply(100); var gain19 = gainanomaly1920.multiply(10); var gain20 = gainanomaly2021.multiply(1); var recoveryFlag = gain16.add(gain17) .add(gain18) .add(gain19) .add(gain20); Several information can be inferred based on the combination of years and each individual flag. For example, one can decide that a plantation class will be anything that shows clearing in one year and recovery in the next; or, two consecutive years of exposed soil followed by 3 years of recovery. Also, extending the anomaly analysis to years prior to 2015, increasing the study period. Thus, it is up to the user to come up to a rule to define the final plantation layer. Finally, as you may have noticed, this technique can also be used to inspect changes elsewhere, chief among them areas of native vegetation that has been cleared. This is a very versitile analysis that can provide very useful information about vegetation change. Export these flags (lossFlag and recoveryFlag): Export.image.toAsset({ image: recoveryFlag, description: &#39;RecoveryFlag&#39;, assetId: &#39;RecoveryFlag&#39;, crs:&#39;EPSG:4326&#39;, crsTransform:[0.0002777777777777778,0,-180.0001388888889,0,-0.0002777777777777778,60.00013888888889], maxPixels:1e13, }); Code Checkpoint 2.2.2 Class-by-class land cover mapping We will start by opening a new code editor page and by defining the spatial and temporal parameters of the composite you wish to classify, just like the other examples. As explained in Part 1, GEEs cloud screening algorithm based on quality assessment bands (QA) can be applied to remove cloud and cloud shadow contaminated pixels for each of the Landsat scene covering y. This method greatly improves the final composite (See 1.43). However, creating an annual completely cloud-free imagery composition for Liberia is a challenging task because of the west African monsoon, which causes constant clouds across the Gulf of Guinea most of the time. The rainy season in Liberia ranges from May to October and it frequently rains in other months, except in the short dry season that runs from December to February/March. Hence, multi-year composites are necessary in attaining wall-to-wall cloud-free mosaics over Liberia. In order to ensure that an adequate cloud-free composite is achieved, we will use a composite circa 2021-2022. Then the temporal parameters can be defined as: var year = 2021; var startDay = (year)+&#39;-01-01&#39;; var endDay = (year+1)+&#39;-05-30&#39;; Note that this time range will include all available Landsat 8 Surface Reflectance images from January 2021 through May 2022 (year+1). For the spatial parameters, we can use the Large Scale International Boundary (LSIB) dataset and select Liberias national border. First, we will load the dataset into an object called 'countries' using the ee.FeatureCollection() object and the feature collection ID 'USDOS/LSIB/2013'. Secondly, we will filter the dataset using .filterMetadata() and select Liberia from the list of countries and storing it in nationalBorder: Note that you can select a any country border by filtering 'USDOS/LSIB/2013' dataset by using .filterMetadata('name' , 'equals', 'NAME OF THE COUNTRY IN CAPITAL LETTERS'). If you want to visualize your feature collection, you can add it to the map using the Map.addLayer(): var countries = ee.FeatureCollection(&#39;USDOS/LSIB/2013&#39;); var nationalBorder = countries.filterMetadata(&#39;name&#39; , &#39;equals&#39;, &#39;LIBERIA&#39;); Map.centerObject(nationalBorder); Map.addLayer(nationalBorder, {color: &#39;Blue&#39;}, &#39;Liberia National Borders&#39;); Figure 2.15: Liberia border extracted from the Large Scale International Boundary (LSIB) dataset. Alternatively, you can import your own border shapefile to your assets using the NEW button in the Assets tab. (See Earth Engine's Importing Raster Data for instructions on uploading an image to your assets or Importing Table/Shapefile Data for more details). In this example, we imported a shapefile containing all of Liberia's counties to our assets: Figure 2.16: Importing shapefiles to your assets. Once the upload is completed, the imported dataset can be accessed in the Assets tab. We will import it to the script using its 'ID' and add to the map editor: var aoi = ee.FeatureCollection(&#39;users/capacityBuilding/Liberia/LiberiaCounties&#39;); Map.addLayer(aoi, {color: &#39;red&#39;}, &#39;LiberiaCounties&#39;); Figure 2.17: aoi and its multiple features (counties) In this particular case, this dataset includes multiple features (or polygons) with several properties for each feature. One of them is called 'NAME_1', which includes the names of each county in Liberia: Figure 2.18: Asset information. This imported dataset includes multiple features. You can also print aoi to the Console tab: print(aoi); Figure 2.19: Feature collection information printed to the console. This imported dataset includes multiple features. This type of metadata can be used to filter a feature collection. The ee.Filter.inList() method is very helpful when you want to filter multiple properties at once. First, we will create an object called 'filterCounties' with a list of 4 of the 15 counties in Liberia. In this example, we selected the counties of Bomi, Montserrado, Margibi and Grand Bassa. Finally, we use filter() to filter aoi for the list of counties provided in filterCounties: var filterCounties = ee.Filter.inList(&#39;NAME_1&#39;, [&#39;Bomi&#39;, &#39;Montserrado&#39;,&#39;Margibi&#39;,&#39;GrandBassa&#39;]); var filteredAOI = aoi.filter(filterCounties); Map.addLayer(filteredAOI, {color: &#39;green&#39;}, &#39;filteredArea&#39;); Figure 2.20: The aoi feature collection filtered by metadata. Here, a filter to select the counties of Bomi, Montserrado, Margibi and Grand Bassa was used. Now that both temporal and spatial parameters are defined, As auxiliary functions for cloud masking and spectral index calculation, we will use the same functions provided in PART 2: function maskClouds(image) { var cloudShadowBitMask = ee.Number(2).pow(3).int(); var cloudsBitMask = ee.Number(2).pow(4).int(); var QA = image.select(&#39;QA_PIXEL&#39;); var mask = QA.bitwiseAnd(cloudShadowBitMask).eq(0) .and(QA.bitwiseAnd(cloudsBitMask).eq(0)); return image.updateMask(mask).divide(100000).select(&quot;SR_B[0-9]*&quot;).copyProperties(image, [&quot;system:time_start&quot;]); } function addIndices(image) { var ndvi = image.normalizedDifference([&#39;SR_B5&#39;,&#39;SR_B4&#39;]).rename(&#39;NDVI&#39;); var nbr = image.normalizedDifference([&#39;SR_B5&#39;,&#39;SR_B7&#39;]).rename(&#39;NBR&#39;); var ndmi = image.normalizedDifference([&#39;SR_B7&#39;,&#39;SR_B3&#39;]).rename(&#39;NDMI&#39;); var mndwi = image.normalizedDifference([&#39;SR_B3&#39;,&#39;SR_B6&#39;]).rename(&#39;MNDWI&#39;); var sr = image.select(&#39;SR_B5&#39;).divide(image.select(&#39;SR_B4&#39;)).rename(&#39;SR&#39;); var bare = image.normalizedDifference([&#39;SR_B6&#39;,&#39;SR_B7&#39;]).rename(&#39;BI&#39;); var gcvi = image.expression(&#39;(NIR/GREEN)-1&#39;,{ &#39;NIR&#39;:image.select(&#39;SR_B5&#39;), &#39;GREEN&#39;:image.select(&#39;SR_B3&#39;) }).rename(&#39;GCVI&#39;); var evi = image.expression( &#39;2.5 * ((NIR-RED) / (NIR + 6 * RED - 7.5* SR_BLUE +1))&#39;, { &#39;NIR&#39;:image.select(&#39;SR_B5&#39;), &#39;RED&#39;:image.select(&#39;SR_B4&#39;), &#39;SR_BLUE&#39;:image.select(&#39;SR_B2&#39;) }).rename(&#39;EVI&#39;); var msavi = image.expression( &#39;(2 * NIR + 1 - sqrt(pow((2 * NIR + 1), 2) - 8 * (NIR - RED)) ) / 2&#39;, { &#39;NIR&#39;: image.select(&#39;SR_B5&#39;), &#39;RED&#39;: image.select(&#39;SR_B4&#39;)} ).rename(&#39;MSAVI&#39;); return image .addBands(ndvi) .addBands(nbr) .addBands(ndmi) .addBands(mndwi) .addBands(sr) .addBands(evi) .addBands(msavi) .addBands(gcvi) .addBands(bare); } 1) Landsat 8 Image Collection and Cloud-free Mosaic Similarly to the examples from PART 2 and PART 3.1, we will load the Landsat 8 Surface Reflectance data archive into an object called 'collection' by using the container ee.ImageCollection() and the collection ID 'LANDSAT/LC08/C02/T1_L2'. Secondly, we will filter this image collection for the temporal parameters by using the method .filterDate(). Finally, we will map the cloud-masking and spectral indices functions to the collection: var collection = ee.ImageCollection(&#39;LANDSAT/LC08/C02/T1_L2&#39;) .filterDate(startDay,endDay) .map(maskClouds) .map(addIndices); Then, in an object called 'composite', we will reduce the image collection into a 2021/2022 composite using the .median() method. Finally, we will clip the composite to our area of interest filteredAOI using the container .clip() method: var composite = collection .median() .clip(filteredAOI); We will add composite to the Map editor: Map.addLayer(composite, {bands: [&#39;SR_B5&#39;, &#39;SR_B6&#39;, &#39;SR_B4&#39;], min:0.1, max:0.2}, &#39;Composite&#39;,false); Figure 2.21: A RGB564 composition of composite. Finally, we can export this composite as an asset. This will save you some time in the next steps where we will be using this same composite for classifying multiple land cover. For this example, we will use the Export.image.toAsset() as follows: Export.image.toAsset({ image: composite, description: &#39;CompositeLiberia2021&#39;, assetId: &#39;CompositeLiberia2021&#39;, crs:&#39;EPSG:4326&#39;, crsTransform:[0.0002777777777777778,0,-180.0001388888889,0,-0.0002777777777777778,60.00013888888889], maxPixels:1e13, }); The code above will export the composite to your asset folder using the coordinate reference system (crs) and its affine. This is a necessary step to avoid a 15 meter pixel shift when using the default export parameters. Choose your folder and your asset ID anc click run: Figure 2.22: Exporting the composite as an asset. Code Checkpoint 2) Classification: Class-by-class A class-by-class classification approach will follow the steps depicted in the figure below: Figure 2.23: A class by class classification approach. So, far we have covered a multi-class classification approach. This is the most common way to perform a classification of multiple classes: feed the classifier all the samples from the different classes and post-process the outputs. However, we can include a pre-classification step to isolate the class of interest and then perform the classification for that particular land cover. Later, we will mask this class from the composite in order to exclude those pixels from the classification process and potentially avoid comission and omission errors. Why is this relevant/important? - Answer: Pixel-based classification may generate a large number of misclassified pixels (the &quot;salt-and-pepper effect&quot;) due to the spectral diversity within the same land cover type and spectral confusion between land cover types. For that, a spectral signature analysis can be performed for identifying potential spectral bands and reflectance-based spectral indices thresholds for differentiating between two classes: the land cover class being mapped and the remaining land cover classes merged as a single class called Other. Water bodies Water bodies are usually the easiest class to start with. Not many other land cover classes will have a similar spectral signature. Therefore, it is good practice to start with classes that are easily classifiable. As mentioned previously, we will use a pre-classification step to isolate the class of interest and then perform the classification for that particular land cover. We can do that by using a Masking Phase (Se Figure 2.23). In this phase, we use a band/ index threshold analysis to create a mask that will remove most pixels that are unlikely to be the class of interest - in this particular case, water bodies. In this example we will use the Modified Normalized Difference Water Index (MNDWI) that was calculated and added as a separate band to the composite using the addIndices function. This index will usually have mostly positive values for wet areas and negative values for other land cover classes. Therefore we can creat a 'wetMask' using a value thresold: var wetMask = composite.select(&#39;MNDWI&#39;).gte(-0.1).clip(filteredAOI); The code above is selecting the pixels from the band &quot;MNDWI&quot; from composite that has values that are equal or greater than -0.1. Automatically, wetMask will be an image of ones (1) where the pixels meet the criteria above (MNDWI values greater than -0.1) and zeros (0) everywhere else. You can add wetMask to the map to check how constrictive your threshold values are: Map.addLayer(wetMask.selfMask(), {palette:&#39;blue&#39;}, &#39;WetMask&#39;); Figure 2.24: A mask created based on the MNDWI values that are equal or greater than -0.1. Note that this mask includes all the water pixels (in black as seen in the RGB 564 color composition) plus other areas that area not open water bodies but include some level of wetness to them (usually mangroves and wetlands). The objective of this phase IS NOT to remove ALL pixels that are not water; rather, is to remove MOST pixels that are not water. You can think of this as a classification process that preceeds the actual random forest classification. Instead of having to classify the entire composite into two classes, this way you can reduce the number of pixels going into the Random Forest Classification, speeding up the process. Also, this process already eliminated pixels that are unlikely to be water, reducing even further the chances of missclassification. We will use this mask to mask composite: var compositemasked = composite.mask(wetMask); Map.addLayer(compositemasked, {bands: [&#39;SR_B5&#39;, &#39;SR_B6&#39;, &#39;SR_B4&#39;], min:0.1, max:0.2}, &#39;Composite Masked&#39;); Figure 2.25: composite masked using wetMask. This is the resulting Landsat composite that will be used in this first classification iteration! The next steps should be familiar to you: * Selecting training samples - here we will create the two geometries to hold the 'water' and 'other' samples. Remember to set a property called 'landcover': Figure 2.26: Geometry imports to hold the training samples. Code Checkpoint Training a random forest classifier: var classes = Water.merge(Other); var bands = [&#39;SR_B4&#39;,&#39;SR_B5&#39;,&#39;SR_B6&#39;,&#39;NDVI&#39;,&#39;NBR&#39;,&#39;MNDWI&#39;,&#39;SR&#39;,&#39;GCVI&#39;,&#39;MSAVI&#39;]; var samples = compositemasked.select(bands).sampleRegions({ collection: classes, properties: [&#39;landcover&#39;], scale: 30 }).randomColumn(&#39;random&#39;); var split = 0.8; var training = samples.filter(ee.Filter.lt(&#39;random&#39;, split)); var testing = samples.filter(ee.Filter.gte(&#39;random&#39;, split)); print(&#39;Samples n =&#39;, samples.aggregate_count(&#39;landcover&#39;)); print(&#39;Training n =&#39;, training.aggregate_count(&#39;landcover&#39;)); print(&#39;Testing n =&#39;, testing.aggregate_count(&#39;landcover&#39;)); var classifier = ee.Classifier.smileRandomForest(100,5).train({ features: training, classProperty: &#39;landcover&#39;, inputProperties: bands }); var validation = testing.classify(classifier); var testAccuracy = validation.errorMatrix(&#39;landcover&#39;, &#39;classification&#39;); print(&#39;Validation error matrix RF: &#39;, testAccuracy); print(&#39;Validation overall accuracy RF in %: &#39;, testAccuracy.accuracy().multiply(100)); Classifying compositemasked into 'Water' and 'Others'. var classification = compositemasked.select(bands).classify(classifier); var paletteMAP = [ &#39;#35ff17&#39;, // Other (Class value 0) &#39;#1c48d6&#39;, // Water (Class value 1) ]; Map.addLayer (classification, {min: 0, max: 1, palette:paletteMAP}, &#39;Classification&#39;); You should get something similar to this: Figure 2.27: Classification output. Finally, we will save this output to our Assets folder using the Export.image.toAsset(): Export.image.toAsset({ image: classification, description: &#39;WaterClassification&#39;, assetId: &#39;1-Water&#39;, crs:&#39;EPSG:4326&#39;, crsTransform:[0.0002777777777777778,0,-180.0001388888889,0,-0.0002777777777777778,60.00013888888889], maxPixels:1e13, }); This export may take several minutes. This asset will be used in the next classification to mask the composite for this class! Code Checkpoint Mangroves Congratulations! You have classified your first land cover class! Next on the list is 'Mangroves'. This one is fairly straighforward to classify as this type of vegetation will have traces of the water spectral signature. We can use a similar index threshold approach for this class. Adittionally, we can use other datasets to mask other pixels that are unlikely to be Mangroves, such as elevation (Please refer to the Guinea section of this chapter for more information). First, we will open a new Code Editor page and import the Landsat composite, the feature collection of Liberia counties and the water classification: var composite = ee.Image(&#39;users/capacityBuilding/Liberia/CompositeLiberia2021&#39;); var aoi = ee.FeatureCollection(&#39;users/capacityBuilding/Liberia/LiberiaCounties&#39;); var filterCounties = ee.Filter.inList(&#39;NAME_1&#39;, [&#39;Bomi&#39;, &#39;Montserrado&#39;,&#39;Margibi&#39;,&#39;GrandBassa&#39;]); var filteredAOI = aoi.filter(filterCounties); var waterClass = ee.Image(&#39;users/capacityBuilding/Liberia/1-Water&#39;); Map.addLayer(composite, {bands: [&#39;SR_B5&#39;, &#39;SR_B6&#39;, &#39;SR_B4&#39;], min:0.1, max:0.2}, &#39;Composite&#39;); In this step, we will create three masks: one based the class we already have (water class),one based on elevation (mangroves tend to ocurr in flat areas near the shore. Thus an elevation cut off can be use to eliminate most of other pixels) and, finally, one based on spectral threshold (similarly to what we have done in the previous step). The resulting composite will be what we will use for classifying Mangroves: Mask I - Water Class Here, we will create a mask based on the water classification. For that, we will create a ee.Image populate it with 1 and where it overlaps with the water class, populate it with 0. This way, when we apply this mask to the composite, the classified water pixels will be masked, leaving only the other pixels available for classification: var waterMask = ee.Image(1). where(waterClass.select([&#39;classification&#39;]).eq(1),0).clip(filteredAOI); Map.addLayer(waterMask.selfMask(), {palette:[&#39;yellow&#39;]}, &#39;Water Mask&#39;); Note that the value provided within eq() needs to match the value of your class. Mask II - Elevation (See 2.4) For the elevation mask, we will take advantage of the Shuttle Radar Topography Mission's elevation data available on GEE. We will use an elevation cut off of 25 meters: the composite will be masked where pixels have values &gt; 25 m of elevation: var elevation = ee.Image(&#39;USGS/SRTMGL1_003&#39;).select(&#39;elevation&#39;); var elevationMask = elevation.lte(25).clip(filteredAOI); Map.addLayer(elevationMask.selfMask(), {palette:[&#39;pink&#39;]}, &#39;Elevation Mask&#39;); Mask III - Index Threshold For the threshold mask, we will use the same index we used in the Water classification masking phase: var wetMask = composite.select(&#39;MNDWI&#39;).gte(-0.12); Map.addLayer(wetMask.selfMask(), {palette:[&#39;blue&#39;]}, &#39;Wet Mask&#39;); These are the three masks that will be applied to composite: Figure 2.28: When applying these three masks to the composite, pixels will only remain where the three masks overlap. Finally, we will apply these masks to the composite using .mask() for the first mask and .updateMask() for the others: var compositemasked = composite.mask(waterMask) .updateMask(elevationMask) .updateMask(wetMask); Map.addLayer(compositemasked, {bands: [&#39;SR_B5&#39;, &#39;SR_B6&#39;, &#39;SR_B4&#39;], min:0.1, max:0.2}, &#39;Composite Masked&#39;); You should achieve something like this: Figure 2.29: Masked composite. Next, we will repeat the sampling selection and classification. As a tip for selection accurate samples, mangroves will be in a darker shade of red/orange in this RGB 564 color composition while other vegetative classes (e.g. evergreen forests) will have a lighter red/orange shade. Make sure to take advantage of the high resolution imagery using the Satellite button: Figure 2.30: Mangroves and other vegetation classes in Liberia. var classes = Mangrove.merge(Other); var bands = [&#39;SR_B4&#39;,&#39;SR_B5&#39;,&#39;SR_B6&#39;,&#39;NDVI&#39;,&#39;NBR&#39;,&#39;MNDWI&#39;,&#39;SR&#39;,&#39;GCVI&#39;,&#39;MSAVI&#39;]; var samples = compositemasked.select(bands).sampleRegions({ collection: classes, properties: [&#39;landcover&#39;], scale: 30 }).randomColumn(&#39;random&#39;); var split = 0.8; var training = samples.filter(ee.Filter.lt(&#39;random&#39;, split)); var testing = samples.filter(ee.Filter.gte(&#39;random&#39;, split)); print(&#39;Samples n =&#39;, samples.aggregate_count(&#39;landcover&#39;)); print(&#39;Training n =&#39;, training.aggregate_count(&#39;landcover&#39;)); print(&#39;Testing n =&#39;, testing.aggregate_count(&#39;landcover&#39;)); var classifier = ee.Classifier.smileRandomForest(100,5).train({ features: training, classProperty: &#39;landcover&#39;, inputProperties: bands }); var validation = testing.classify(classifier); var testAccuracy = validation.errorMatrix(&#39;landcover&#39;, &#39;classification&#39;); print(&#39;Validation error matrix RF: &#39;, testAccuracy); print(&#39;Validation overall accuracy RF in %: &#39;, testAccuracy.accuracy().multiply(100)); var classification = compositemasked.select(bands).classify(classifier); var paletteMAP = [ &#39;#35ff17&#39;, // Other (Class value 0) &#39;#ce0dd6&#39;, // Mangroves (Class value 1) ]; Map.addLayer (classification, {min: 0, max: 1, palette:paletteMAP}, &#39;Classification&#39;); Figure 2.31: Classification output. Finally, we will export the classification: Export.image.toAsset({ image: classification, description: &#39;MangroveClassification&#39;, assetId: &#39;2-Mangroves&#39;, crs:&#39;EPSG:4326&#39;, crsTransform:[0.0002777777777777778,0,-180.0001388888889,0,-0.0002777777777777778,60.00013888888889], maxPixels:1e13, }); Code Checkpoint Settlements Now, we are moving into classes that require not only a spectral analysis but also a contextual one to be accurately classified. A good example is the artificial surfaces/ settlements class. In the case of Liberia, these areas will have very similar spectral behavior as exposed soil and bare areas, as most of the villages and small towns are usually located within these land cover classes. Therefore, a contextual evaluation needs to be included to ensure that bare areas are not being missclassified as settlements. In this case, we can use datasets like the location of these settlements and population density. These are extra pieces of information that we can use to constrain the classification to the correct area. In this example, we will use the Liberia census population dataset from 2007-2008 created by Liberia Institute of Statistics and Geo-Information Services (LISGIS). The dataset contains a list of all the settlements that are geo-located and have attributes with their administrative units and population data (total, male, female and number of households). Additionally, we can use the Global Population Density dataset to further refine our classification. Similarly to the last classification, we will open a new Code Editor page and import the Landsat composite, the feature collection of Liberia counties, the feature collection of Liberia settlements, the global population density dataset (available through GEE) and the water and mangrove classification outputs: var composite = ee.Image(&#39;users/capacityBuilding/Liberia/CompositeLiberia2021&#39;); var aoi = ee.FeatureCollection(&#39;users/capacityBuilding/Liberia/LiberiaCounties&#39;); var filterCounties = ee.Filter.inList(&#39;NAME_1&#39;, [&#39;Bomi&#39;, &#39;Montserrado&#39;,&#39;Margibi&#39;,&#39;GrandBassa&#39;]); var filteredAOI = aoi.filter(filterCounties); var settlements = ee.FeatureCollection(&#39;users/capacityBuilding/Liberia/PopulatedSettlements2007&#39;); var PopulationCount = ee.Image(&#39;JRC/GHSL/P2016/POP_GPW_GLOBE_V1/2015&#39;); var waterClass = ee.Image(&#39;users/capacityBuilding/Liberia/1-Water&#39;); var mangroveClass = ee.Image(&#39;users/capacityBuilding/Liberia/2-Mangroves&#39;); You can add these layers to the Map Editor for inspection: Map.addLayer(composite, {bands: [&#39;SR_B5&#39;, &#39;SR_B6&#39;, &#39;SR_B4&#39;], min:0.1, max:0.2}, &#39;Composite&#39;); Map.addLayer(PopulationCount.clip(filteredAOI), {palette:[&quot;ffffe7&quot;,&quot;FFc869&quot;,&quot;ffac1d&quot;,&quot;e17735&quot;,&quot;f2552c&quot;,&quot;9f0c21&quot;], min:0, max:500}, &#39;Population Count&#39;); Map.addLayer(settlements, {}, &#39;Settlements&#39;); Figure 2.32: Ancillary datasets for classification of human settlements in Liberia. The population density and small settlement locations is helpful to constrain the classification in certain areas and help reduce misclassification errors with spectrally similar classes such as bare soil. Similarly to the previous classification, we will create the masks based on these datasets to apply to the composite: Mask I - Water and Mangrove class masks var waterMask = ee.Image(1). where(waterClass.select([&#39;classification&#39;]).eq(1),0).clip(filteredAOI); var mangroveMask = ee.Image(1). where(mangroveClass.select([&#39;classification&#39;]).eq(1),0).clip(filteredAOI); Mask II - Population Masks In this particular case, we are using two different datasets: one is an ee.Image() (PopulationCount) and the other is a ee.FeatureCollection() (settlements). Fot that we will create two different masks and merge them together as a single mask. First, we will create a population count mask based on the number of people per pixel. Using the same construction as before, we will create a ee.Image() of value 1 and populate it with zeros where PopulationCount is equal or lower than 100 people. In other words, we are creating a mask to remove pixels where there is less than 100 people, which is an indication that this area may not be a fully established settlement: var popMask = ee.Image(1). where(PopulationCount.select([&#39;population_count&#39;]).lte(100),0).clip(filteredAOI); Map.addLayer(popMask.selfMask(), {palette:[&#39;green&#39;]}, &#39;Population Mask&#39;); Figure 2.33: Population mask created by selecting areas that have more than 100 people/pixel. Once this mask is applied, areas with less than 100 people per pixel will be masked. Next, we will use settlements. As you may have noticed, settlements contains the location of each small human settlement based on the 2007 census. We can then create a buffer of a certain size around each of these locations to create areas of likelihood for human settlements. We will first create a function to apply a buffer of a 1000 meters around the feature and then map it to settlements using .map() method: var bufferSet = function(feature) { return feature.buffer(1000); }; var setLocation = settlements.map(bufferSet); Map.addLayer(setLocation, {color:&#39;purple&#39;}, &#39;Settlements&#39;); Figure 2.34: 1000 m buffer around each settlement location. Now, we can merge them into a final mask by creating an ee.Image() of 1 and clip it using the buffered locations, the .clip() method and add it to popMask using .add(): var populationMask = ee.Image(1).clip(setLocation).unmask().add(popMask); Map.addLayer(populationMask.selfMask(), {palette:&#39;blue&#39;}, &#39;Population Mask Final&#39;); Note that the code above used .unmask() after .clip(). In order to mathematically add two images, they have to have values. .clip() will cut an image for the extent of the feature used. Therefore, the rest will be automatically masked. Using .unmask() allows you to reaply 0 values to the masked areas, allowing you to do mathematical operations with it! Figure 2.35: Final population Mask. Finally, we will apply these masks to the composite: var compositemasked = composite.mask(waterMask) .updateMask(mangroveMask) .updateMask(populationMask); Map.addLayer(compositemasked, {bands: [&#39;SR_B5&#39;, &#39;SR_B6&#39;, &#39;SR_B4&#39;], min:0.1, max:0.2}, &#39;Composite Masked&#39;); Figure 2.36: Masked composite using waterMask, mangroveMask and populationMask. Mask III - Index Threshold As an additional mask, you can create a NBR-based mask to only highlight areas that are exposed and has no vegetation to it. Since we already applied the population and location based masks, this one can further remove extra pixels that are not likely to be human settlements, for instance, highly vegetated areas: var bareMask = compositemasked.select(&#39;NBR&#39;).lte(0.25); var compositeMasked = compositemasked.updateMask(bareMask); Map.addLayer(compositeMasked, {bands: [&#39;SR_B5&#39;, &#39;SR_B6&#39;, &#39;SR_B4&#39;], min:0.1, max:0.2}, &#39;Composite Masked Final&#39;); Figure 2.37: Masked composite using waterMask, mangroveMask and populationMask and bareMask showing areas of higher likelihood to be human settlements class. Code Checkpoint Similarly to the other classes, we will collect samples of urban areas and other classes and train a classifier. Finally, We can apply the random forest classifier to this resulting composite. var classes = City.merge(Other); var bands = [&#39;SR_B4&#39;,&#39;SR_B5&#39;,&#39;SR_B6&#39;,&#39;NDVI&#39;,&#39;NBR&#39;,&#39;MNDWI&#39;,&#39;SR&#39;,&#39;GCVI&#39;,&#39;MSAVI&#39;]; var samples = compositeMasked.select(bands).sampleRegions({ collection: classes, properties: [&#39;landcover&#39;], scale: 30 }).randomColumn(&#39;random&#39;); var split = 0.8; var training = samples.filter(ee.Filter.lt(&#39;random&#39;, split)); var testing = samples.filter(ee.Filter.gte(&#39;random&#39;, split)); print(&#39;Samples n =&#39;, samples.aggregate_count(&#39;landcover&#39;)); print(&#39;Training n =&#39;, training.aggregate_count(&#39;landcover&#39;)); print(&#39;Testing n =&#39;, testing.aggregate_count(&#39;landcover&#39;)); var classifier = ee.Classifier.smileRandomForest(100,5).train({ features: training, classProperty: &#39;landcover&#39;, inputProperties: bands }); var validation = testing.classify(classifier); var testAccuracy = validation.errorMatrix(&#39;landcover&#39;, &#39;classification&#39;); print(&#39;Validation error matrix RF: &#39;, testAccuracy); print(&#39;Validation overall accuracy RF in %: &#39;, testAccuracy.accuracy().multiply(100)); var classification = compositeMasked.select(bands).classify(classifier); var paletteMAP = [ &#39;#ffc82d&#39;, // Other (Class value 0) &#39;#0c4f95&#39;, // Settlements (Class value 1) ]; Map.addLayer (classification, {min: 0, max: 1, palette:paletteMAP}, &#39;Classification&#39;); Figure 2.38: Classification Output. Code Checkpoint Grasslands/ Herbaceous Vegetation After classifying Human Settlements, the next class we will be focusing on is Grasslands/ Herbaceous Vegetation. Similarly to the previous class, this one have a spectral signature that follows the overall behavior of bare soil. Barren Lands and Grasslands are likely to be very heterogeneous and often show intermixing with other land cover classes and with each other. In this case, we can take advantage of bareness-related indices to highlight these areas and use them as potential masks to assist in the classification. For example, the Normalized Burn Ratio is often used to highlight burned and bare areas: Figure 2.39: The Normalized Burn Ratio and Grasslands. Areas with little to no vegetation cover is shown in black. First, we will start importing the same assets as before and the classes we have so far: water bodies, mangroves and human settlements: var composite = ee.Image(&#39;users/capacityBuilding/Liberia/CompositeLiberia2021&#39;); var aoi = ee.FeatureCollection(&#39;users/capacityBuilding/Liberia/LiberiaCounties&#39;); var filterCounties = ee.Filter.inList(&#39;NAME_1&#39;, [&#39;Bomi&#39;, &#39;Montserrado&#39;,&#39;Margibi&#39;,&#39;GrandBassa&#39;]); var filteredAOI = aoi.filter(filterCounties); var waterClass = ee.Image(&#39;users/capacityBuilding/Liberia/1-Water&#39;); var mangroveClass = ee.Image(&#39;users/capacityBuilding/Liberia/2-Mangroves&#39;); var settlementClass = ee.Image(&#39;users/capacityBuilding/Liberia/3-Settlements&#39;); Next, we will create the masks based on the classes we have so far and applying to the composite: var waterMask = ee.Image(1). where(waterClass.select([&#39;classification&#39;]).eq(1),0).clip(filteredAOI); var mangroveMask = ee.Image(1). where(mangroveClass.select([&#39;classification&#39;]).eq(1),0).clip(filteredAOI); var settlementMask = ee.Image(1). where(settlementClass.select([&#39;classification&#39;]).eq(1),0).clip(filteredAOI); var compositemasked = composite.mask(waterMask) .updateMask(mangroveMask) .updateMask(settlementMask); Map.addLayer(compositemasked, {bands: [&#39;SR_B5&#39;, &#39;SR_B6&#39;, &#39;SR_B4&#39;], min:0.1, max:0.2}, &#39;Composite Masked&#39;); At this point, the composite is masked for three classes and what is left is mostly vegetative classes (ever green forests and plantation) with the exception of little to no vegetation cover classes such as herbaceous vegetation/ grasslands and bare areas. For these low biomass/bare classes, we can still use the random forest classification route: collecting training samples and classifying composite. However, due to their distinct spectral signature compare do their surrounding classes and their distinct pattern (particularly in the area highlighted in this example), you may even be able to separate them with index threshold. In this example, we will use this technique for the sake of demonstration. We will use the NBR index as an example: The lower the values of NBR, the more exposed/bare that particular area is. Therefore, after inspecting the values over areas of bare soil and grasslands, we concluded that bare soil had, in average, lower values than 0.05, while grasslands had between 0.05 and 0.2. Remember to always use the Inspector tab if you want to learn more about the values of a particular pixel in that image! You can check the average value of pixels within a region by using ee.ReduceRegions() and ee.Reducer.mean(): var averageNBR = compositemasked.select(&#39;NBR&#39;).reduceRegions({ collection: Bare.geometry(), reducer: ee.Reducer.mean(), scale: 30, }); print(averageNBR) Thus we can apply the conditionals .lte() and .gt() to the NBR band from composite to extract these classes: var bareClass = compositemasked.select(&#39;NBR&#39;).lte(0.05).rename(&#39;bare&#39;); Map.addLayer(bareClass.selfMask(), {palette:&#39;red&#39;}, &#39;bareClass&#39;); For grasslands, we can combine conditionals using .and(): var grassClass = compositemasked.select(&#39;NBR&#39;).gt(0.05).and(compositemasked.select(&#39;NBR&#39;).lte(0.2)).rename(&#39;grass&#39;); Map.addLayer(grassClass.selfMask(), {palette:&#39;green&#39;}, &#39;grassClass&#39;); Figure 2.40: Extracting the herbaceous vegetation/grass and bare areas using NBR thresholds. Finally, we will export these classes to the assets following the same code as before: Export.image.toAsset({ image: bareClass, description: &#39;BareClassification&#39;, assetId: &#39;4-Bareareas&#39;, crs:&#39;EPSG:4326&#39;, crsTransform:[0.0002777777777777778,0,-180.0001388888889,0,-0.0002777777777777778,60.00013888888889], maxPixels:1e13, }); Export.image.toAsset({ image: grassClass, description: &#39;GrassClassification&#39;, assetId: &#39;5-Grasslands&#39;, crs:&#39;EPSG:4326&#39;, crsTransform:[0.0002777777777777778,0,-180.0001388888889,0,-0.0002777777777777778,60.00013888888889], maxPixels:1e13, }); Code Checkpoint Plantation Cultivated land in Liberia primarily consists of woody commodity crops (oil palm and rubber trees). Figure 2.41: Rubber (red) and Oil Palm (green) concessions in Liberia. The spectral signature of these woody crops are identical with those of natural vegetation making it very challenging to classify. As seen in the previous section (3.2.1), areas of plantation usually undergo harvesting cycles, which creates a temporal signature that is different than evergreen natural vegetation. We took the advantage of this interannual differences by doing an anomaly analysis and creating losses/harvest and gains/planting flag layers. We will start by opening a new Code Editor page and importing the flag layers, the composite and the Liberia counties feature collection: var lossFlag = ee.Image(&#39;users/capacityBuilding/Liberia/LossFlag&#39;); var recoveryFlag = ee.Image(&#39;users/capacityBuilding/Liberia/RecoveryFlag&#39;); var composite = ee.Image(&#39;users/capacityBuilding/Liberia/CompositeLiberia2021&#39;); var aoi = ee.FeatureCollection(&#39;users/capacityBuilding/Liberia/LiberiaCounties&#39;); var filterCounties = ee.Filter.inList(&#39;NAME_1&#39;, [&#39;Bomi&#39;, &#39;Montserrado&#39;,&#39;Margibi&#39;,&#39;GrandBassa&#39;]); var filteredAOI = aoi.filter(filterCounties); These flag layers have pixels with 5 digit combinations of 0 and 1 value representing absence and presence, respectively It is up to the user to come up to a rule to define the final plantation layer based on the combination of years (see section (3.2.1)). Examples of this would be pixels that showed losses for at least 2 consecutive years, no matter what year: 11000 - 2016 and 2017 01100 - 2017 and 2018 00110 - 2018 and 2019 00011 - 2019 and 2020 11100 - 2016, 2017 and 2018 01110 - 2017, 2018 and 2019 00111 - 2018, 2019 and 2020 11110 - 2016, 2017, 2018 and 2019 01111 - 2017, 2018, 2019 and 2020 Or selecting pixels that showed losses on consecutive years: var disturbance1year = lossFlag.eq(10000); // 2016 var disturbance2years = lossFlag.eq(11000);// 2016, 2017 var disturbance3years = lossFlag.eq(11100);// 2016, 2017, 2018 var disturbance4years = lossFlag.eq(11110);// 2016, 2017, 2018, 2019 var disturbance5years = lossFlag.eq(11111);// 2016, 2017, 2018, 2019, 2020 Map.addLayer(lossFlag.selfMask(), {palette:[&#39;white&#39;]}, &#39;LossFlag&#39;); Map.addLayer(disturbance1year.selfMask(), {palette:[&#39;E0C097&#39;]}, &#39;Disturbance 1 Year&#39;); Map.addLayer(disturbance2years.selfMask(), {palette:[&#39;D79771&#39;]}, &#39;Disturbance 2 Years&#39;); Map.addLayer(disturbance3years.selfMask(), {palette:[&#39;B85C38&#39;]}, &#39;Disturbance 3 Years&#39;); Map.addLayer(disturbance4years.selfMask(), {palette:[&#39;753422&#39;]}, &#39;Disturbance 4 Years&#39;); Map.addLayer(disturbance5years.selfMask(), {palette:[&#39;2D2424&#39;]}, &#39;Disturbance All Years&#39;); Figure 2.42: The loss flag (in white) and selected flag values. For this example, we will consider plantation, every pixel that shows at least 1 year of loss/harvest and at least 1 year of gain/planting in those flags. In other words, both loss/harvest and gain/planting flags. Thus, we will add lossFlag and recoveryFlag, create an image of 0 and where the sum overlaps this image, populate it with 1. Finally clip it for the area of study (filteredAOI): var plantations = ee.Image(0).where(lossFlag.add(recoveryFlag),1).clip(filteredAOI); Map.addLayer(plantations.selfMask(), {palette:[&#39;green&#39;]}, &#39;Plantation&#39;); Figure 2.43: Plantation class derived from the lossFlag and recoveryFlag One last step remains to finalize the plantation class layer: Remember that every other class prior to this one was created from a classified Landsat composite that was masked each time using the previous class. Therefore, we need to mask plantations so it matches with how it would have been if it were created with the Landsat composite. Similarly to the previous classes, we will add the class assets, and use them to create masks for plantations: var waterClass = ee.Image(&#39;users/capacityBuilding/Liberia/1-Water&#39;); var mangroveClass = ee.Image(&#39;users/capacityBuilding/Liberia/2-Mangroves&#39;); var settlementClass = ee.Image(&#39;users/capacityBuilding/Liberia/3-Settlements&#39;); var bareClass = ee.Image(&#39;users/capacityBuilding/Liberia/4-Bareareas&#39;); var grassClass = ee.Image(&#39;users/capacityBuilding/Liberia/5-Grasslands&#39;); var waterMask = ee.Image(1). where(waterClass.select([&#39;classification&#39;]).eq(1),0).clip(filteredAOI); var mangroveMask = ee.Image(1). where(mangroveClass.select([&#39;classification&#39;]).eq(1),0).clip(filteredAOI); var settlementMask = ee.Image(1). where(settlementClass.select([&#39;classification&#39;]).eq(1),0).clip(filteredAOI); var bareMask = ee.Image(1). where(bareClass.select([&#39;bare&#39;]).eq(1),0).clip(filteredAOI); var grassMask = ee.Image(1). where(grassClass.select([&#39;grass&#39;]).eq(1),0).clip(filteredAOI); Then, mask plantations using the masks with the method mask() for the first and .updateMask() for the rest: var plantationClass = plantations.mask(waterMask) .updateMask(mangroveMask) .updateMask(settlementMask) .updateMask(bareMask) .updateMask(grassMask) .rename (&#39;plantations&#39;); It is a good practice to use .rename() to give a name to the image you just created. If you choose not to rename your image, its band name will default to 'constant'. Finally, export this layer as an asset: Export.image.toAsset({ image: plantationClass, description: &#39;plantationClass&#39;, assetId: &#39;6-Plantations&#39;, crs:&#39;EPSG:4326&#39;, crsTransform:[0.0002777777777777778,0,-180.0001388888889,0,-0.0002777777777777778,60.00013888888889], region: filteredAOI, maxPixels:1e13, }); Code checkpoint Forests According to a report by the World Bank, &quot;Liberia is one of the most forested countries in West Africa, with more than two thirds of its land surface covered by forest. The National Forest Inventory, conducted by the Liberia Forestry Development Authority in 2018 and 2019, estimates the forest cover in Liberia to be 6.69 million hectares which is approximately 69 percent of the total landmass.&quot; Thus, it comes as no surprise that the vast majority of the Landsat composite we have been using is comprised of tree covered areas. So far, we have have 6 land cover and land-use classes: water bodies, mangroves, human settlements, bare areas, grasslands and plantations. Then, the remaining of the composite is covered by trees. According to Liberia's forest definition, forests are areas of at least 0.5 ha covered by trees that have more than 30% canopy cover and are at least 5 meters of height. Forests can be classified in different ways and to different degrees of specificity. For this example, we will categorize the tree-cover/forest class in Liberia into three sub-classes: primary/dense, secondary/open and degraded/sparse. Separating different types of forests and/or different forest condition (e.g. primary and secondary) can become increasingly challenging because: (1) sometimes it requires the incorporation of complex physical models associated with specific forest types, making it difficult for widespread use, and (2) it requires multi-source high resolution auxiliary data such as 3D vegetation structure data. However, 3D vegetation structure datasets are vastly more scarce than spaceborne 2D data and, in most cases, only covers certain periods of time. In a previous study, we investigated the integrity of forests in Liberia in 2018, using the Global Ecosystem Dynamics Investigation (GEDI)'s L2B Canopy Cover and Vertical Profile metrics acquired between April 2018 and November 2020, available from the NASA/USGS Land Processes Distributed Active Archive Center (DAAC). At each GEDI footprint we extracted the relative height metric RH100 (100th percentile of beam return height in meters relative to the ground) and total canopy cover (percent) gridded to the Landsat 30-meter pixel. Using these metrics, we observed that the distribution of canopy height values of open/secondary forests will be shorter than older dense/primary forests. Similarly, open/secondary forests will be characterized by lower values of canopy cover compared to dense/primary forests. Finally, spase/degraded forests will have lower canopy cover and height values compared to the other classes: Figure 2.44: GEDI-based measurements of Liberias forests integrity. Canopy height (in meters) and canopy cover (in %) was extracted from each GEDI footprint within dense tree-covered areas (a), open tree-covered areas (b), mixed vegetation (c) and all other classes (d). The summary statistics for the canopy cover and canopy height values for each class (and all other classes) is also shown (SD = standard deviation; N = count; SE = standard error of the mean). We can use the information above to derive primary/dense, secondary/open and degraded/sparse based on height values or canopy cover. The 2019 Potapovs (University of Maryland) global canopy height dataset offers a 30-m spatial resolution global forest canopy height map and was developed through the integration of the Global Ecosystem Dynamics Investigation (GEDI) lidar forest structure measurements and Landsat analysis-ready data time-series. Our previous study on forest structure in Liberia and this dataset are a good starting point to categorize the tree-cover/forest classes in Liberia. First, we will start by importing all the relevant datasets into a new Code Editor and creating their respective masks: var composite = ee.Image(&#39;users/capacityBuilding/Liberia/CompositeLiberia2021&#39;); var aoi = ee.FeatureCollection(&#39;users/capacityBuilding/Liberia/LiberiaCounties&#39;); var filterCounties = ee.Filter.inList(&#39;NAME_1&#39;, [&#39;Bomi&#39;, &#39;Montserrado&#39;,&#39;Margibi&#39;,&#39;GrandBassa&#39;]); var filteredAOI = aoi.filter(filterCounties); Map.addLayer(composite, {bands: [&#39;SR_B5&#39;, &#39;SR_B6&#39;, &#39;SR_B4&#39;], min:0.1, max:0.2}, &#39;Composite&#39;, false); var waterClass = ee.Image(&#39;users/capacityBuilding/Liberia/1-Water&#39;); var mangroveClass = ee.Image(&#39;users/capacityBuilding/Liberia/2-Mangroves&#39;); var settlementClass = ee.Image(&#39;users/capacityBuilding/Liberia/3-Settlements&#39;); var bareClass = ee.Image(&#39;users/capacityBuilding/Liberia/4-Bareareas&#39;); var grassClass = ee.Image(&#39;users/capacityBuilding/Liberia/5-Grasslands&#39;); var plantationClass = ee.Image(&#39;users/capacityBuilding/Liberia/6-Plantations&#39;); var waterMask = ee.Image(1). where(waterClass.select([&#39;classification&#39;]).eq(1),0).clip(filteredAOI); var mangroveMask = ee.Image(1). where(mangroveClass.select([&#39;classification&#39;]).eq(1),0).clip(filteredAOI); var settlementMask = ee.Image(1). where(settlementClass.select([&#39;classification&#39;]).eq(1),0).clip(filteredAOI); var bareMask = ee.Image(1). where(bareClass.select([&#39;bare&#39;]).eq(1),0).clip(filteredAOI); var grassMask = ee.Image(1). where(grassClass.select([&#39;grass&#39;]).eq(1),0).clip(filteredAOI); var plantationMask = ee.Image(1). where(plantationClass.select([&#39;plantations&#39;]).eq(1),0).clip(filteredAOI); Next, we will import the 2019 Potapovs (University of Maryland) global canopy height dataset available through 'users/potapovpeter/GEDI_V27/GEDI_SAFR_v27' var potapovTreeHeight = ee.Image(&#39;users/potapovpeter/GEDI_V27/GEDI_SAFR_v27&#39;).clip(filteredAOI); This dataset needs to match the projection we are using for our composite and land cover classes. Thus, we will use .reproject() to adjust for their projection differences. var treeHeight = potapovTreeHeight.reproject(composite.projection()); The .projection() method above retrives an image projection and affine transformation parameters. Thus, here we are using the Landsat composite projection information and reprojecting potapovTreeHeight to match it. Add treeHeight to the Map Editor using the parameters below: Map.addLayer(treeHeight, {min:0, max:30, palette:&#39;ffffcc,c7e9b4,7fcdbb,41b6c4,1d91c0,225ea8,0c2c84&#39;}, &#39;Tree Height Potapov&#39;,false); Figure 2.45: 2019 Potapovs (University of Maryland) global canopy height dataset over the study region in Liberia. Tree heights values straches from low (white) to high (dark blue). Similarly to what was done to the plantation class, we need to mask this dataset for the classes that we already have: var TreeHeightMasked = treeHeight.mask(waterMask) .updateMask(mangroveMask) .updateMask(settlementMask) .updateMask(bareMask) .updateMask(grassMask) .updateMask(plantationMask) We can now extract potential areas of primary/dense, secondary/open and degraded/sparse forests based on the canopy height thresholds identified in our previous study: primary/dense forests are taller than 20 meters, secondary/open ranging from 10 to 20 meters, and degraded/sparse forests ar shorter than 10 meters: var denseForest = TreeHeightMasked.gte(20).rename(&#39;dense&#39;); var openForest = TreeHeightMasked.gte(10).and(TreeHeightMasked.lt(20)).rename(&#39;open&#39;); var degradedForest = TreeHeightMasked.lte(10).rename(&#39;degraded&#39;); Map.addLayer(denseForest.selfMask(), {palette:&#39;287300&#39;},&#39;Potential Mature&#39;); Map.addLayer(openForest.selfMask(), {palette:&#39;73B473&#39;},&#39;Potential Secondary&#39;); Map.addLayer(degradedForest.selfMask(), {palette:&#39;EBFFBE&#39;},&#39;Potential Degraded&#39;); Figure 2.46: Primary/dense, secondary/open and degraded/sparse forests based on the canopy height thresholds. Export these classes following the steps we used so far: Export.image.toAsset({ image: denseForest, description: &#39;DenseForest&#39;, assetId: &#39;7-DenseForests&#39;, crs:&#39;EPSG:4326&#39;, crsTransform:[0.0002777777777777778,0,-180.0001388888889,0,-0.0002777777777777778,60.00013888888889], region: filteredAOI, maxPixels:1e13, }); Export.image.toAsset({ image: openForest, description: &#39;OpenForest&#39;, assetId: &#39;8-OpenForests&#39;, crs:&#39;EPSG:4326&#39;, crsTransform:[0.0002777777777777778,0,-180.0001388888889,0,-0.0002777777777777778,60.00013888888889], region: filteredAOI, maxPixels:1e13, }); Export.image.toAsset({ image: degradedForest, description: &#39;DegradedForest&#39;, assetId: &#39;9-DegradedForests&#39;, crs:&#39;EPSG:4326&#39;, crsTransform:[0.0002777777777777778,0,-180.0001388888889,0,-0.0002777777777777778,60.00013888888889], region: filteredAOI, maxPixels:1e13, }); Code Checkpoint{target=&quot;_blank&quot;&quot;} Post Classification I - Map Composition Now that all the individual class maps were created, we can combine them into a final land cover map. We will start by opening a New Code Editor page and adding all the assets we created so far: var aoi = ee.FeatureCollection(&#39;users/capacityBuilding/Liberia/LiberiaCounties&#39;); var filterCounties = ee.Filter.inList(&#39;NAME_1&#39;, [&#39;Bomi&#39;, &#39;Montserrado&#39;,&#39;Margibi&#39;,&#39;GrandBassa&#39;]); var filteredAOI = aoi.filter(filterCounties); var waterClass = ee.Image(&#39;users/capacityBuilding/Liberia/1-Water&#39;); var mangroveClass = ee.Image(&#39;users/capacityBuilding/Liberia/2-Mangroves&#39;); var settlementClass = ee.Image(&#39;users/capacityBuilding/Liberia/3-Settlements&#39;); var bareClass = ee.Image(&#39;users/capacityBuilding/Liberia/4-Bareareas&#39;); var grassClass = ee.Image(&#39;users/capacityBuilding/Liberia/5-Grasslands&#39;); var plantationClass = ee.Image(&#39;users/capacityBuilding/Liberia/6-Plantations&#39;); var denseClass = ee.Image(&#39;users/capacityBuilding/Liberia/7-DenseForests&#39;); var openClass = ee.Image(&#39;users/capacityBuilding/Liberia/8-OpenForests&#39;); var degradedClass = ee.Image(&#39;users/capacityBuilding/Liberia/9-DegradedForests&#39;); Using the same logic we used to create the masks for masking the composites, we will create each individual class layer and assign a class value to each one of them. First we will create an ee.Image() of zeros where a different value will be assigned where a particular class overlaps it. For example: var WATER = ee.Image(0).where(waterClass.select([&#39;classification&#39;]).eq(1),1).clip(filteredAOI); var MANGROVES = ee.Image(0). where(mangroveClass.select([&#39;classification&#39;]).eq(1),2).clip(filteredAOI); var SETTLEMENTS = ee.Image(0). where(settlementClass.select([&#39;classification&#39;]).eq(1),3).clip(filteredAOI); var BARE = ee.Image(0). where(bareClass.select([&#39;bare&#39;]).eq(1),4).clip(filteredAOI); var GRASSLAND = ee.Image(0). where(grassClass.select([&#39;grass&#39;]).eq(1),5).clip(filteredAOI); var PLANTATION = ee.Image(0). where(plantationClass.select([&#39;plantations&#39;]).eq(1),6).clip(filteredAOI); var DENSE = ee.Image(0). where(denseClass.select([&#39;dense&#39;]).eq(1),7).clip(filteredAOI); var OPEN = ee.Image(0). where(openClass.select([&#39;open&#39;]).eq(1),8).clip(filteredAOI); var DEGRADED = ee.Image(0). where(degradedClass.select([&#39;degraded&#39;]).eq(1),9).clip(filteredAOI); In the code above, two things are worth noting: The .select() method will need the name of the output in which to select the value. If the output was created using the Random Forest, its default name will be 'classification'. The first three classes were created using the classifier. The following classes were created using value thresholds of other datasets. In this case their name defaults to 'constant'. However, during the process, we used .rename() method to rename 'constant' to the name of the class. We are assigning consecutive integer values from 1 to 9 to each individual classes. This way, we can colorize the map using a color palette A final map can be achieved by adding all these layers together using .add(): var finalMap = WATER.add(MANGROVES) .add(SETTLEMENTS) .add(BARE) .add(GRASSLAND) .add(PLANTATION) .add(DENSE) .add(OPEN) .add(DEGRADED); Finally, using a color palette of your choice, we will add the map to the Map Editor: var palette = [ &#39;006EFA&#39;, // Water Bodies &#39;C800FF&#39;, // Mangroves and Wetlands &#39;FF0000&#39;, // Artificial Surfaces &#39;828282&#39;, // Baresoil &#39;A5FF73&#39;, // Grasslands &#39;FFAA00&#39;, // Woody Crops &#39;287300&#39;, // Dense Forests &#39;73B473&#39;, // Open Forests &#39;EBFFBE&#39;, // Degraded Forests ]; Map.addLayer(finalMap, {min: 1, max: 9, palette: palette}, &#39;Map&#39;); As an add-on, you can add a legend to the Map Editor using the code below: var legend = ui.Panel({ style: { position: &#39;bottom-left&#39;, padding: &#39;8px 15px&#39; } }); var legendTitle = ui.Label({ value: &#39;Land cover classes&#39;, style: { fontWeight: &#39;bold&#39;, fontSize: &#39;15px&#39;, margin: &#39;0 0 4px 0&#39;, padding: &#39;0&#39;, } }); legend.add(legendTitle); var makeRow = function(color, name) { var colorBox = ui.Label({ style: { backgroundColor: color, padding: &#39;8px&#39;, margin: &#39;0 0 4px 0&#39; } }); var description = ui.Label({ value: name, style: {margin: &#39;0 0 4px 6px&#39;} }); return ui.Panel({ widgets: [colorBox, description], layout: ui.Panel.Layout.Flow(&#39;horizontal&#39;) }); }; var names = [&#39;Water Bodies&#39;, &#39;Mangroves and Wetlands&#39;, &#39;Artificial Surfaces&#39;, &#39;Bare Soil&#39;, &#39;Grasslands&#39;, &#39;Woody Crops&#39;, &#39;Dense Forests&#39;, &#39;Open Forests&#39;, &#39;Degraded Forests&#39;, ]; for (var i = 0; i &lt; 9; i++) { legend.add(makeRow(palette[i], names[i])); } Map.add(legend); Figure 2.47: Final land cover map for the study area in Liberia. The classes were created individually and composed into a final map. For the following step, we will export this final map using the code below: Export.image.toAsset({ image: finalMap, description: &#39;ExportingLCM&#39;, assetId: &#39;CompositedMap&#39;, scale: 30, region: filteredAOI, maxPixels:1e13 }); Code Checkpoint Post Classification II - Map Refinement As mentioned in section 2.3.2, a common issue that arises from the pixel-based classification of fine/medium spatial resolution imagery is the salt-and-pepper effect. This happens when individual pixels are classified differently from their neighbors, creating speckles of isolated pixels of different classes. In this section, we will focus on a post-classification technique to reduce the salt and pepper effect and edge roughness of the map we have just created. Opening a New Code Editor page, we will add the exported land cover map and add it to the Map Editor using the color palette: var landcovermap = ee.Image(&#39;users/capacityBuilding/Liberia/CompositedMap&#39;); var palette = [ &#39;006EFA&#39;, // Water Bodies &#39;C800FF&#39;, // Mangroves and Wetlands &#39;FF0000&#39;, // Artificial Surfaces &#39;828282&#39;, // Baresoil &#39;A5FF73&#39;, // Grasslands &#39;FFAA00&#39;, // Woody Crops &#39;287300&#39;, // Dense Forests &#39;73B473&#39;, // Open Forests &#39;EBFFBE&#39;, // Degraded Forests ]; Map.addLayer(landcovermap, {min: 1, max: 9, palette: palette}, &#39;Map&#39;); Zooming in, you will notice the salt-and-pepper effect: Figure 2.48: Final land cover map for the study area in Liberia. The the salt-and-pepper effect is a common issue that arises from the pixel-based classification of fine/medium spatial resolution imagery. Using the techniques discussed in section 2.3.2, we will remove the salt and pepper effect in three steps: First, we will use the .connectedPixelCount() method. This generates an image where each pixel contains the number of 4- or 8-connected neighbors (including itself). 4-connected pixels are neighbors to every pixel that touches one of their edges. These pixels are connected horizontally and vertically. var patchsize = landcovermap.connectedPixelCount(4, false); Second, we will use the .focal_median() method. This method is a simple sliding-window spatial filter that replaces the center value (or class) in the window with the median of all the pixel values in the window. var filtered = landcovermap.focal_median({ radius: 1, kernelType: &#39;square&#39;, units: &#39;pixels&#39;, }); Finally, we will replace isolated pixels in landcovermap based on the patchsize value of 1: this value means that this pixel have only 1 type (or class) neighbors which, in turn, means that it is surrounded by a single class. We will use the .where() and .eq() methods. The .focal_median(1, 'square', 'pixels') will only achieve the intended final result if you reproject it back to the original scale and projection from the original map. Note that this technique while effective in removing the &quot;salt-and-pepper&quot; effect from the original map, it creates edges that are not in the same resolution and projection as the original map. Therefore, creating fairly rounded boundaries for the classes. This is not ideal as it is usually intended for these maps to keep the same resolution as the original map, as well as to preserve the boundaries between classes. To account for this issue, you can reproject this output back the original scale so it is formed by 30 x 30 meter pixels using .reproject() var filteredMap = landcovermap.where(patchsize.eq(1),filtered).reproject(ee.Image(landcovermap.projection())); Add filteredMap to the Map Editor: Map.addLayer(filteredMap, {min: 1, max: 9, palette: palette}, &#39;Filtered Map&#39;); Figure 2.49: Final land cover map for the study area in Liberia filtered to remove salt and pepper effect. "],["part-4-ecosystem-accounting.html", "Chapter 3 Part 4 - Ecosystem Accounting 3.1 Introduction to Ecosystem Accounting 3.2 Ecosystem Extent Account 3.3 Tools for Compiling Ecosystem Extent Account Tables 3.4 Compiling Ecosystem Extent Account Tables", " Chapter 3 Part 4 - Ecosystem Accounting 3.1 Introduction to Ecosystem Accounting In this module we will learn about the System of Environmental Economic Accounting--Ecosystem Accounting (SEEA EA) -- the accounting standard developed for ecosystem accounting. It is a shortened version of a course developed by the United Nations Statistical Division focusing on ecosystem extent accounts. The full course can be accessed at the UN Global Learning Platform. 3.1.1 Essentials A) What is the SEEA EA? SEEA EA is the international statistical standard for organizing data about ecosystems, measuring ecosystem services, tracking changes in ecosystem assets, and linking this information to economic and other human activity. It was adopted by the UN Statistical Commission in March 2021. The aim of the SEEA EA is to systematically record data on the stocks and flows related to ecosystems. It provides a framework for measuring ecosystem services, extent and condition, valuing ecosystem services and assets and linking this information to the economy. The SEEA EA is comprehensive and covers all terrestrial, freshwater, marine and subterranean ecosystems. The SEEA is primarily intended to support national level policy decision making, but implementation can be flexible and modular. For example, ecosystem accounts can be used to support decision making for individual administrative areas such as counties and districts, and for environmentally defined areas such as water catchments, protected areas and coastal zones. B) Ecosystem accounts The SEEA EA consists of a system of integrated, internally consistent series of accounts that cover different dimensions of how an ecosystem can be described and measured: Figure 3.1: Components of ecosystem accounting. C) Ecosystem assets The primary spatial units for ecosystem accounting are ecosystem assets. Ecosystem assets are contiguous spaces of a specific ecosystem type characterized by a distinct set of biotic and abiotic components and their interactions: &lt;Figure&gt; Information about ecosystem assets includes their extent, condition, and services they provide. This information may be expressed in physical and/or monetary units. D) Ecosystem types For the purpose of accounting, all ecosystem assets need to be classified into ecosystem types. It is recommended that existing national ecosystem classifications schemes be used wherever possible. However, cross-referencing of national classifications with the IUCN Global Ecosystem Typology (IUCN GET) will enable accounts to be compared between countries. &lt;Figure&gt; The IUCN GET comprises six hierarchical levels ranging from broad classes that differentiate the functional properties of ecosystems to finer levels of ecosystem types that will be relevant in national and sub-national scales. It is recommended that compilation of ecosystem accounts at the national level should take place for ecosystem functional groups (level 3) or even finer classification reflecting data availability and policy demands. &lt;Figure&gt; E) Ecosystem extent Ecosystem extent is the size of an ecosystem asset. It is usually measured in terms of area but may also be measured in terms of length or volume in case of rivers and lakes. In practice, basic spatial units (BSU) -- geometrical constructs representing a small spatial area -- are used for measuring the size of ecosystem assets. For example we use grid cells (pixels) from a satellite image or other BSU shapes such as polygons, lines or points. The size of BSU is smaller or equal in size to any ecosystem asset. &lt;Figure&gt; F) Ecosystem extent account Ecosystem extent accounts represent a common starting point for ecosystem accounting by providing information about the area, composition of, and changes in, ecosystem types within an ecosystem accounting area (EAA). At the national level, The EAA extends to cover all terrestrial, freshwater and marine ecosystems within the country's boarders and its marine exclusive economic zone (EEZ). In practice, EAAs may be smaller in geographical size covering just terrestrial or marine realm or a sub-national region such as a coastal zone or a protected area. 3.2 Ecosystem Extent Account In this section we will learn about the basic structure of ecosystem extent accounts. The compilation process will be demonstrated on simple examples followed by practical exercises. 3.2.1 Structure of ecosystem extent account A) Ecosystem extent (units of area) -- Table 1 B) Ecosystem type change matrix -- Table 2 3.2.2 Examples and exercises A) Ecosystem extent (units of area) -- Table 1 B) Ecosystem type change matrix -- Table 2 3.3 Tools for Compiling Ecosystem Extent Account Tables This section demonstrates the process of deriving ecosystem extent account tables using Python scripts developed to run within Google Earth Engine Python Colab and Jupyter Notebook with ArcGIS libraries. 3.3.1 Platforms for running Python scripts A) GEE Python Colab Google Earth Engine Application Programming Interface (API) Colab, or &quot;Colaboratory&quot; is a web-based interactive development environment that allows you to write and execute Python in your browser with zero configuration required, and an access to graphics processing units (GPU) free of charge. B) Jupyter Notebook Jupyter Notebook is the latest web-based interactive development environment for notebooks, code, and data. Its flexible interface allows users to configure and arrange workflows in data science, scientific computing, computational journalism, and machine learning. It was designed to work with a large number of libraries including those developed for ArcGIS. 3.4 Compiling Ecosystem Extent Account Tables This section explores the use of Microsoft Excel to do basic data analytics using information from ecosystem extent accounts. 3.4.1 Essentials A) Macros If you have tasks in Microsoft Excel that you do repeatedly, you can record a macro to automate those tasks. A macro is an action or a set of actions that you can run as many times as you want. When you create a macro, you are recording your mouse clicks and keystrokes. "]]
